{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# trovare le parole piÃ¹ frequenti per ogni sentimento\n",
    "\n",
    "import nltk\n",
    "\n",
    "from src.Tweet import TweetInfo, Tweet, lex_resources_directory, tweets_directory\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "# Se ne abbiamo voglia possiamo mettere le emoticons e gli emoji su file e per poi leggerli\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "import pymongo\n",
    "import mariadb\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Class used to store information about each tweet: its sentiment e how many words of which sentiment are in that tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class LexicalResource:\n",
    "    filename: str\n",
    "    sentiment: str\n",
    "    word_list: List[str]\n",
    "\n",
    "    def __init__(self, filename: str, sentiment: str):\n",
    "        self.filename = filename\n",
    "        self.sentiment = sentiment\n",
    "        self.word_list = []\n",
    "\n",
    "    def __str__(self):\n",
    "        lex_res_string = \"LexicalResource: \" + self.filename + \\\n",
    "                         \"\\n\\t sentiment: \" + self.sentiment + \\\n",
    "                         \"\\n\\t wordlist: \" + self.word_list.__str__()\n",
    "        return lex_res_string\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        if not '_' in word:\n",
    "            self.word_list.append(word)\n",
    "\n",
    "    def add_word_list(self, word_list: List[str]):\n",
    "        for word in word_list:\n",
    "            if not '_' in word:\n",
    "                self.word_list.append(word)\n",
    "\n",
    "    def get_number_of_words(self) -> int:\n",
    "        return len(self.word_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline popofewf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Read files in directory\n",
    "General function to read text files from a directory and merge them"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "lex_resources_list: List[LexicalResource] = []\n",
    "\n",
    "def read_texts_in_directory(directory_path: str, sentiment: str) -> List[str]:\n",
    "    files_text_list: List[str] = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            file = open(file=file_path, encoding=\"utf8\")\n",
    "            file_text = file.read().split() # list of words for a single lex resource of a sentiment\n",
    "            files_text_list = files_text_list + file_text\n",
    "\n",
    "            lex_res: LexicalResource = LexicalResource(filename, sentiment)\n",
    "            lex_res.add_word_list(file_text)\n",
    "            global lex_resources_list\n",
    "            lex_resources_list.append(lex_res)\n",
    "\n",
    "    # print(len(lex_resources_list))\n",
    "    # [print(i) for i in lex_resources_list]\n",
    "    return files_text_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Read lexical resources for a sentiment\n",
    "Function which reads all the lexical resources for a sentiment\n",
    "The directory containing all lexical resources for that sentiment is passed as parameter\n",
    "Returns a set of the words in all the lexical resources of a sentiment\n",
    "### forse creare per ogni lex res di OGNI sentimento un dizionario diverso? Bisogna vedere come caricare i dati su db, bisogna caricare ogni lex res diversa di ogni sentimento sul db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def read_lex_resources_sentiment(sentiment_lex_resources_directory: str, sentiment: str) -> Set[str]:\n",
    "    resource_words: Set[str] = set()\n",
    "    resources_text: List[str] = read_texts_in_directory(sentiment_lex_resources_directory, sentiment)\n",
    "    for word in resources_text:\n",
    "        if not '_' in word:\n",
    "            resource_words.add(word)\n",
    "    #print(sentiment, \"\\n\", resource_words, \"\\n\\n\")\n",
    "    return resource_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read all lexical resources\n",
    "\n",
    "Reads all the lexical resources and returns a dictionary of word to sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bu': 'Anger', 'bubu': 'Anger', 'pensa': 'Joy', 'ya': 'Anger', 'no': 'Anger', 'bbu': 'Anger', 'angry': 'Anger', 'yes': 'Joy', 'privacy': 'Joy', 'boella': 'Joy'}\n"
     ]
    }
   ],
   "source": [
    "sentiment_lex_resources: Dict[str, str] = {}\n",
    "\n",
    "for resources_path, sentiments, _ in os.walk(lex_resources_directory):\n",
    "    # The folders inside the lexical resources folder are named after a sentiment (Ex. Anger, Joy), each of them contain some files and each of them is a list of words that are associated with that sentiment\n",
    "    \"\"\"WARN Ad ogni ciclo ci sarebbe per forza un singolo sentiment dato che cicla sulla directory delle directory di lex res?\"\"\"\n",
    "    for sentiment in sentiments:\n",
    "        # iterate each folder (one for sentiment)\n",
    "        resources_sentiment_path = os.path.join(resources_path, sentiment)\n",
    "        sentiment_words_set: Set[str] = read_lex_resources_sentiment(resources_sentiment_path, sentiment)\n",
    "\n",
    "        # read the files containing lists of words, and return a set of all the words in those files\n",
    "        for sentiment_word in sentiment_words_set:\n",
    "            # associate each word of the set to the corresponding sentiment\n",
    "            sentiment_lex_resources[sentiment_word] = sentiment\n",
    "\n",
    "lex_word_to_sentiment = sentiment_lex_resources\n",
    "print(lex_word_to_sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tweet reading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Reads a file and converts the text to tweets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def read_tweet_file(file_path_string: str, sentiment: str) -> List[Tweet]:\n",
    "    \"\"\"\n",
    "    Reads a file and converts the text to tweets\n",
    "    :param file_path_string: string of the path to the file\n",
    "    \"\"\"\n",
    "\n",
    "    # tweets read from file\n",
    "    tweets_read: List[Tweet] = []\n",
    "\n",
    "    tweets_file = open(file=file_path_string, encoding=\"utf8\")\n",
    "    tweets_text: List[str] = tweets_file.readlines()\n",
    "\n",
    "    # For each tweet text create a Tweet object\n",
    "    i = 0\n",
    "    for tweet_text in tweets_text:\n",
    "        i = i + 1\n",
    "        new_tweet = Tweet(tweet_text, i, sentiment)\n",
    "        tweets_read.append(new_tweet)\n",
    "\n",
    "    return tweets_read"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get list of sentiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "sentiments: List[str] = [sentiment for sentiment in os.listdir(lex_resources_directory)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read tweets folder and load Tweet Info for stem counting\n",
    "The tweets folder contains for each sentiment a file containing tweets of that sentiment. Each file is scanned and for each tweet a TweetInfo object is created in order to maintain the count of how many word of which sentiments are in it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ma associare il TweetInfo al tweet senza fare un altro dict?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_tweet_sentiment_from_file_name(file_name: str):\n",
    "    extension_removed = file_name.split(\".\")[0]\n",
    "    sentiment = extension_removed.split(\"_\")[-2]\n",
    "    return sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "tweets_to_info: Dict[Tweet, TweetInfo] = {}\n",
    "for tweets_sentiments_directory, _, tweets_sentiments_filenames in os.walk(tweets_directory):\n",
    "    # print(tweets_sentiments_directory)\n",
    "    # print(tweets_sentiments_files)\n",
    "\n",
    "    for tweets_sentiment_filename in tweets_sentiments_filenames:\n",
    "        # print(tweets_sentiment_file)\n",
    "        tweets_sentiment_filepath = os.path.join(tweets_sentiments_directory, tweets_sentiment_filename)\n",
    "        sentiment = get_tweet_sentiment_from_file_name(tweets_sentiment_filename)\n",
    "        tweets_for_sentiment: List[Tweet] = read_tweet_file(tweets_sentiment_filepath, sentiment)\n",
    "        #print(\"Tweets for sentiment: \", sentiment, \"\\n\")\n",
    "        for tweet in tweets_for_sentiment:\n",
    "            tweet_info: TweetInfo = TweetInfo(sentiment, sentiments)\n",
    "            tweet.tweet_stem_count = TweetInfo\n",
    "            tweets_to_info[tweet] = tweet_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stem counting\n",
    "For each tweet and each word of them is checked the sentiment and increased the counter for that sentiment in the TweetInfo object associated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "for tweet in tweets_to_info:\n",
    "    tweet_info = tweets_to_info[tweet]\n",
    "    tweet_words: List[str] = tweet.get_words()\n",
    "\n",
    "    for word in tweet_words:\n",
    "        if word in lex_word_to_sentiment:\n",
    "            # get the sentiment for the word and increase sentiment counter by 1\n",
    "            sentiment = lex_word_to_sentiment[word]\n",
    "            tweet_info.increase_sentiment_counter(sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test print"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def print_tweets():\n",
    "    for tweet in tweets_to_info.keys():\n",
    "        info = tweets_to_info[tweet]\n",
    "        print(tweet)\n",
    "        print(\"sentiment: \" + info.sentiment)\n",
    "        print(\"sentiment occurrences: \")\n",
    "        print(info.sentiment_occurrences)\n",
    "        print(\"---\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet\n",
      "\ttweet raw:  yes the cillo is very chill smoking on the balcony ;)\n",
      "\tpos tags: {\"yes\": \"RB\", \"cillo\": \"NN\", \"chill\": \"JJ\", \"smoking\": \"VBG\", \"balcony\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 1}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: wow i'm having fun doing the smoking\n",
      "\tpos tags: {\"wow\": \"NN\", \"'m\": \"VBP\", \"fun\": \"NN\", \"smoking\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: yea  yea\n",
      "\tpos tags: {\"yea\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: boss\tpos tags: {\"boss\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: angry pensa is angry sad banana no\n",
      "\tpos tags: {\"angry\": \"JJ\", \"pensa\": \"NN\", \"sad\": \"JJ\", \"banana\": \"NN\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 3, 'Joy': 1}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: angry boella no pensa kill ;( yoyou ah rip bu\n",
      "\tpos tags: {\"angry\": \"JJ\", \"boella\": \"NN\", \"pensa\": \"NN\", \"kill\": \"NN\", \"ah\": \"VBP\", \"rip\": \"JJ\", \"bu\": \"NN\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 3, 'Joy': 2}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw:  know what she ain't ð don't even need to say it !\tpos tags: {\"know\": \"VB\", \"ai\": \"VBP\", \"n't\": \"RB\", \"\\ud83d\\ude12\": \"NNP\", \"even\": \"RB\", \"need\": \"VB\", \"say\": \"VB\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print_tweets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MongoDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tweets', 'LexResourcesWords', 'LexResources']\n",
      "LexicalResource: angry\n",
      "\t sentiment: Anger\n",
      "\t wordlist: ['no', 'pensa', 'angry', 'bu']\n",
      "LexicalResource: pensa_angry.txt\n",
      "\t sentiment: Anger\n",
      "\t wordlist: ['bbu', 'bubu', 'ya', 'bu']\n",
      "LexicalResource: happy\n",
      "\t sentiment: Joy\n",
      "\t wordlist: ['yes', 'boella', 'privacy', 'pensa']\n"
     ]
    },
    {
     "data": {
      "text/plain": "[None, None, None]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_connected = False\n",
    "if not already_connected:\n",
    "    mongo_client = pymongo.MongoClient(\"mongodb+srv://Peppino:wHzr79JxnRUgK52@cluster0.zkagq.mongodb.net/?retryWrites=true&w=majority\")\n",
    "    mydb = mongo_client[\"maadb_tweets\"]\n",
    "    print(mydb.list_collection_names())\n",
    "\n",
    "coll_list = mydb.list_collection_names()\n",
    "\n",
    "\n",
    "[print(i) for i in lex_resources_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check in which resources each word is contained\n",
    "Creates a dictionary <word, lex_res_list> to map each word with the lexical resources which contains the word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bu': ['angry', 'pensa_angry.txt'], 'bubu': ['pensa_angry.txt'], 'pensa': ['angry', 'happy'], 'ya': ['pensa_angry.txt'], 'no': ['angry'], 'bbu': ['pensa_angry.txt'], 'angry': ['angry'], 'yes': ['happy'], 'privacy': ['happy'], 'boella': ['happy']}\n"
     ]
    }
   ],
   "source": [
    "map_word_lex_res: Dict[str, List[str]] = {}\n",
    "\n",
    "for word in lex_word_to_sentiment:\n",
    "    for lex_res in lex_resources_list:\n",
    "        if word in lex_res.word_list:\n",
    "            if map_word_lex_res.get(word) is None:\n",
    "                map_word_lex_res[word] = [lex_res.filename]\n",
    "            else:\n",
    "                map_word_lex_res[word].append(lex_res.filename)\n",
    "\n",
    "print(map_word_lex_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flags to manage queries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "delete_lex_res = True\n",
    "insert_lex_res = True\n",
    "\n",
    "delete_lex_res_words = True\n",
    "insert_lex_res_words = True\n",
    "\n",
    "delete_tweets = True\n",
    "insert_tweets = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete Lexical Resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n",
      "pensa_angry.txt\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "db_lex_res_collection = mydb[\"LexResources\"]\n",
    "\n",
    "if delete_lex_res:\n",
    "    db_lex_res_collection.delete_many({})\n",
    "\n",
    "if insert_lex_res:\n",
    "    for lex_res in lex_resources_list:\n",
    "        to_upload = {\"_id\" : lex_res.filename,\n",
    "                     \"sentiment\": lex_res.sentiment,\n",
    "                     \"totNumberWords\" : lex_res.get_number_of_words()}\n",
    "        inserted_lex_res = db_lex_res_collection.insert_one(to_upload)\n",
    "        print(inserted_lex_res.inserted_id)\n",
    "        # until here inserted lexical resources basic information in LexRes collection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete words of lexical resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lemma': 'bu', 'resources': [{'$ref': 'LexResources', '$id': 'angry'}, {'$ref': 'LexResources', '$id': 'pensa_angry.txt'}], '_id': ObjectId('62b46624c937a094f6bbe330')}\n",
      "{'lemma': 'bubu', 'resources': [{'$ref': 'LexResources', '$id': 'pensa_angry.txt'}], '_id': ObjectId('62b46624c937a094f6bbe331')}\n",
      "{'lemma': 'pensa', 'resources': [{'$ref': 'LexResources', '$id': 'angry'}, {'$ref': 'LexResources', '$id': 'happy'}], '_id': ObjectId('62b46624c937a094f6bbe332')}\n",
      "{'lemma': 'ya', 'resources': [{'$ref': 'LexResources', '$id': 'pensa_angry.txt'}], '_id': ObjectId('62b46624c937a094f6bbe333')}\n",
      "{'lemma': 'no', 'resources': [{'$ref': 'LexResources', '$id': 'angry'}], '_id': ObjectId('62b46624c937a094f6bbe334')}\n",
      "{'lemma': 'bbu', 'resources': [{'$ref': 'LexResources', '$id': 'pensa_angry.txt'}], '_id': ObjectId('62b46624c937a094f6bbe335')}\n",
      "{'lemma': 'angry', 'resources': [{'$ref': 'LexResources', '$id': 'angry'}], '_id': ObjectId('62b46624c937a094f6bbe336')}\n",
      "{'lemma': 'yes', 'resources': [{'$ref': 'LexResources', '$id': 'happy'}], '_id': ObjectId('62b46624c937a094f6bbe337')}\n",
      "{'lemma': 'privacy', 'resources': [{'$ref': 'LexResources', '$id': 'happy'}], '_id': ObjectId('62b46624c937a094f6bbe338')}\n",
      "{'lemma': 'boella', 'resources': [{'$ref': 'LexResources', '$id': 'happy'}], '_id': ObjectId('62b46624c937a094f6bbe339')}\n",
      "\n",
      "\n",
      " {'bu': ObjectId('62b46624c937a094f6bbe330'), 'bubu': ObjectId('62b46624c937a094f6bbe331'), 'pensa': ObjectId('62b46624c937a094f6bbe332'), 'ya': ObjectId('62b46624c937a094f6bbe333'), 'no': ObjectId('62b46624c937a094f6bbe334'), 'bbu': ObjectId('62b46624c937a094f6bbe335'), 'angry': ObjectId('62b46624c937a094f6bbe336'), 'yes': ObjectId('62b46624c937a094f6bbe337'), 'privacy': ObjectId('62b46624c937a094f6bbe338'), 'boella': ObjectId('62b46624c937a094f6bbe339')}\n"
     ]
    }
   ],
   "source": [
    "map_lex_word_db_id: Dict[str, int] = {}\n",
    "db_lex_res_words_collection = mydb[\"LexResourcesWords\"]\n",
    "\n",
    "if delete_lex_res_words:\n",
    "    db_lex_res_words_collection.delete_many({})\n",
    "\n",
    "if insert_lex_res_words:\n",
    "    # for each word in all the lexical resources insert in LexResWords the word and a\n",
    "    # list of pairs <$ref, $id> to track in which LexRes the word is contained\n",
    "    for word in lex_word_to_sentiment:\n",
    "        list_lex_res = map_word_lex_res[word] # list of lexical resources in which the word is contained\n",
    "        resources = [] # list of pairs to insert in LexResWords\n",
    "\n",
    "        for res in list_lex_res: # populate list adding, one at a time, the lexical resources in which the word is contained\n",
    "            resources.append({\"$ref\": \"LexResources\", \"$id\": res})\n",
    "\n",
    "        word_to_upload = {\"lemma\" : word,\n",
    "                          \"resources\" : resources}\n",
    "        inserted_lex_res_word = db_lex_res_words_collection.insert_one(word_to_upload)\n",
    "        map_lex_word_db_id[word] = inserted_lex_res_word.inserted_id # save object id to use it later to reference resources words from tweet words\n",
    "        print(word_to_upload)\n",
    "\n",
    "print(\"\\n\\n\", map_lex_word_db_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet\n",
      "\ttweet raw:  yes the cillo is very chill smoking on the balcony ;)\n",
      "\tpos tags: {\"yes\": \"RB\", \"cillo\": \"NN\", \"chill\": \"JJ\", \"smoking\": \"VBG\", \"balcony\": \"NN\"}\n",
      "\n",
      "{'sentiment': 'cillo', 'index': 1, 'words': [{'lemma': 'yes', 'POS': 'RB', 'freq': 1, 'in_lex_resources': {'$ref': 'LexResourcesWords', '$id': ObjectId('62b46624c937a094f6bbe337')}}, {'lemma': 'cillo', 'POS': 'NN', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'chill', 'POS': 'JJ', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'smoking', 'POS': 'VBG', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'balcony', 'POS': 'NN', 'freq': 1, 'in_lex_resources': 'None'}], 'hashtags': [], 'emojis': [], 'emoticons': [';)'], '_id': ObjectId('62b46624c937a094f6bbe33a')} \n",
      "\n",
      "\n",
      "Tweet\n",
      "\ttweet raw: wow i'm having fun doing the smoking\n",
      "\tpos tags: {\"wow\": \"NN\", \"'m\": \"VBP\", \"fun\": \"NN\", \"smoking\": \"NN\"}\n",
      "\n",
      "{'sentiment': 'cillo', 'index': 2, 'words': [{'lemma': 'wow', 'POS': 'NN', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': \"'m\", 'POS': 'VBP', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'fun', 'POS': 'NN', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'smoking', 'POS': 'NN', 'freq': 1, 'in_lex_resources': 'None'}], 'hashtags': [], 'emojis': [], 'emoticons': [], '_id': ObjectId('62b46624c937a094f6bbe33b')} \n",
      "\n",
      "\n",
      "Tweet\n",
      "\ttweet raw: yea  yea\n",
      "\tpos tags: {\"yea\": \"NN\"}\n",
      "\n",
      "{'sentiment': 'cillo', 'index': 3, 'words': [{'lemma': 'yea', 'POS': 'NN', 'freq': 2, 'in_lex_resources': 'None'}], 'hashtags': [], 'emojis': [], 'emoticons': [], '_id': ObjectId('62b46624c937a094f6bbe33c')} \n",
      "\n",
      "\n",
      "Tweet\n",
      "\ttweet raw: boss\tpos tags: {\"boss\": \"NN\"}\n",
      "\n",
      "{'sentiment': 'cillo', 'index': 4, 'words': [{'lemma': 'boss', 'POS': 'NN', 'freq': 1, 'in_lex_resources': 'None'}], 'hashtags': [], 'emojis': [], 'emoticons': [], '_id': ObjectId('62b46624c937a094f6bbe33d')} \n",
      "\n",
      "\n",
      "Tweet\n",
      "\ttweet raw: angry pensa is angry sad banana no\n",
      "\tpos tags: {\"angry\": \"JJ\", \"pensa\": \"NN\", \"sad\": \"JJ\", \"banana\": \"NN\"}\n",
      "\n",
      "{'sentiment': 'pensa', 'index': 1, 'words': [{'lemma': 'angry', 'POS': 'JJ', 'freq': 1, 'in_lex_resources': {'$ref': 'LexResourcesWords', '$id': ObjectId('62b46624c937a094f6bbe336')}}, {'lemma': 'pensa', 'POS': 'NN', 'freq': 1, 'in_lex_resources': {'$ref': 'LexResourcesWords', '$id': ObjectId('62b46624c937a094f6bbe332')}}, {'lemma': 'sad', 'POS': 'JJ', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'banana', 'POS': 'NN', 'freq': 1, 'in_lex_resources': 'None'}], 'hashtags': [], 'emojis': [], 'emoticons': [], '_id': ObjectId('62b46624c937a094f6bbe33e')} \n",
      "\n",
      "\n",
      "Tweet\n",
      "\ttweet raw: angry boella no pensa kill ;( yoyou ah rip bu\n",
      "\tpos tags: {\"angry\": \"JJ\", \"boella\": \"NN\", \"pensa\": \"NN\", \"kill\": \"NN\", \"ah\": \"VBP\", \"rip\": \"JJ\", \"bu\": \"NN\"}\n",
      "\n",
      "{'sentiment': 'pensa', 'index': 2, 'words': [{'lemma': 'angry', 'POS': 'JJ', 'freq': 1, 'in_lex_resources': {'$ref': 'LexResourcesWords', '$id': ObjectId('62b46624c937a094f6bbe336')}}, {'lemma': 'boella', 'POS': 'NN', 'freq': 1, 'in_lex_resources': {'$ref': 'LexResourcesWords', '$id': ObjectId('62b46624c937a094f6bbe339')}}, {'lemma': 'pensa', 'POS': 'NN', 'freq': 1, 'in_lex_resources': {'$ref': 'LexResourcesWords', '$id': ObjectId('62b46624c937a094f6bbe332')}}, {'lemma': 'kill', 'POS': 'NN', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'ah', 'POS': 'VBP', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'rip', 'POS': 'JJ', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'bu', 'POS': 'NN', 'freq': 1, 'in_lex_resources': {'$ref': 'LexResourcesWords', '$id': ObjectId('62b46624c937a094f6bbe330')}}], 'hashtags': [], 'emojis': [], 'emoticons': [';('], '_id': ObjectId('62b46624c937a094f6bbe33f')} \n",
      "\n",
      "\n",
      "Tweet\n",
      "\ttweet raw:  know what she ain't ð don't even need to say it !\tpos tags: {\"know\": \"VB\", \"ai\": \"VBP\", \"n't\": \"RB\", \"\\ud83d\\ude12\": \"NNP\", \"even\": \"RB\", \"need\": \"VB\", \"say\": \"VB\"}\n",
      "\n",
      "{'sentiment': 'pensa', 'index': 3, 'words': [{'lemma': 'know', 'POS': 'VB', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'ai', 'POS': 'VBP', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': \"n't\", 'POS': 'RB', 'freq': 2, 'in_lex_resources': 'None'}, {'lemma': 'ð', 'POS': 'NNP', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'even', 'POS': 'RB', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'need', 'POS': 'VB', 'freq': 1, 'in_lex_resources': 'None'}, {'lemma': 'say', 'POS': 'VB', 'freq': 1, 'in_lex_resources': 'None'}], 'hashtags': [], 'emojis': ['ð'], 'emoticons': [], '_id': ObjectId('62b46624c937a094f6bbe340')} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_tweets_collection = mydb[\"Tweets\"]\n",
    "\n",
    "if delete_tweets:\n",
    "    db_tweets_collection.delete_many({})\n",
    "\n",
    "if insert_tweets:\n",
    "    for tweet in tweets_to_info:\n",
    "        tweet_words_upload = []\n",
    "        for word in tweet.pos_tags:\n",
    "            if map_lex_word_db_id.get(word) is None:\n",
    "                # Decide what to do with words that do not have a lexical resource associated, we could think about associating it to a resource or some other strategy.\n",
    "                tweet_words_upload.append({\n",
    "                    \"lemma\": word,\n",
    "                    \"POS\": tweet.pos_tags[word],\n",
    "                    \"freq\": tweet.word_frequency[word],\n",
    "                    \"in_lex_resources\" : \"None\"})\n",
    "            else:\n",
    "                tweet_words_upload.append({\n",
    "                    \"lemma\": word,\n",
    "                    \"POS\": tweet.pos_tags[word],\n",
    "                    \"freq\": tweet.word_frequency[word],\n",
    "                    \"in_lex_resources\" : {\"$ref\": \"LexResourcesWords\", \"$id\": map_lex_word_db_id[word]}})\n",
    "\n",
    "        tweet_to_upload = {\n",
    "            \"sentiment\": tweet.sentiment,\n",
    "            \"index\": tweet.index,\n",
    "            \"words\" : tweet_words_upload,\n",
    "            \"hashtags\" : tweet.hashtags,\n",
    "            \"emojis\" : tweet.emojis,\n",
    "            \"emoticons\" : tweet.emoticons}\n",
    "\n",
    "        inserted_tweets = db_tweets_collection.insert_one(tweet_to_upload)\n",
    "\n",
    "        print(tweet)\n",
    "        print(tweet_to_upload, \"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# db_tweets_collection = mydb[\"Tweets\"]\n",
    "#\n",
    "# if delete_tweets:\n",
    "#     db_tweets_collection.delete_many({})\n",
    "#\n",
    "# if insert_tweets:\n",
    "#     for tweet in tweets_to_info:\n",
    "#         tweet_words_upload = []\n",
    "#         for word in tweet.pos_tags:\n",
    "#             if map_lex_word_db_id.get(word) is None:\n",
    "#                 # Decide what to do with words that do not have a lexical resource associated, we could think about associating it to a resource or some other strategy.\n",
    "#                 tweet_words_upload.append({\n",
    "#                     \"lemma\": word,\n",
    "#                     \"POS\": tweet.pos_tags[word],\n",
    "#                     \"freq\": tweet.word_frequency[word],\n",
    "#                     \"in_lex_resources\" : \"None\"})\n",
    "#             else:\n",
    "#                 tweet_words_upload.append({\n",
    "#                     \"lemma\": word,\n",
    "#                     \"POS\": tweet.pos_tags[word],\n",
    "#                     \"freq\": tweet.word_frequency[word],\n",
    "#                     \"in_lex_resources\" : {\"$ref\": \"LexResourcesWords\", \"$id\": map_lex_word_db_id[word]}})\n",
    "#\n",
    "#         tweet_to_upload = {\n",
    "#             \"sentiment\": tweet.sentiment,\n",
    "#             \"index\": tweet.index,\n",
    "#             \"words\" : tweet_words_upload,\n",
    "#             \"hashtags\" : tweet.hashtags,\n",
    "#             \"emojis\" : tweet.emojis,\n",
    "#             \"emoticons\" : tweet.emoticons}\n",
    "#\n",
    "#         inserted_tweets = db_tweets_collection.insert_one(tweet_to_upload)\n",
    "#\n",
    "#         print(tweet)\n",
    "#         print(tweet_to_upload, \"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MariaDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error connecting to MariaDB Platform: Can't connect to server on 'localhost' (10061)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperationalError\u001B[0m                          Traceback (most recent call last)",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 5\u001B[0m     conn \u001B[38;5;241m=\u001B[39m \u001B[43mmariadb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mroot\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43marmando12\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhost\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlocalhost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43mport\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3306\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdatabase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaadb_tweets\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m mariadb\u001B[38;5;241m.\u001B[39mError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mOperationalError\u001B[0m: Can't connect to server on 'localhost' (10061)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mSystemExit\u001B[0m                                Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError connecting to MariaDB Platform: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m     \u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Get Cursor\u001B[39;00m\n",
      "\u001B[1;31mSystemExit\u001B[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1983\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   1980\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m exception_only:\n\u001B[0;32m   1981\u001B[0m     stb \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAn exception has occurred, use \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mtb to see \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1982\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthe full traceback.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m-> 1983\u001B[0m     stb\u001B[38;5;241m.\u001B[39mextend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mInteractiveTB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_exception_only\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1984\u001B[0m \u001B[43m                                                     \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1985\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1986\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1987\u001B[0m         \u001B[38;5;66;03m# Exception classes can customise their traceback - we\u001B[39;00m\n\u001B[0;32m   1988\u001B[0m         \u001B[38;5;66;03m# use this in IPython.parallel for exceptions occurring\u001B[39;00m\n\u001B[0;32m   1989\u001B[0m         \u001B[38;5;66;03m# in the engines. This should return a list of strings.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:585\u001B[0m, in \u001B[0;36mListTB.get_exception_only\u001B[1;34m(self, etype, value)\u001B[0m\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_exception_only\u001B[39m(\u001B[38;5;28mself\u001B[39m, etype, value):\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001B[39;00m\n\u001B[0;32m    579\u001B[0m \n\u001B[0;32m    580\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    583\u001B[0m \u001B[38;5;124;03m    value : exception value\u001B[39;00m\n\u001B[0;32m    584\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 585\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mListTB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:443\u001B[0m, in \u001B[0;36mListTB.structured_traceback\u001B[1;34m(self, etype, evalue, etb, tb_offset, context)\u001B[0m\n\u001B[0;32m    440\u001B[0m     chained_exc_ids\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;28mid\u001B[39m(exception[\u001B[38;5;241m1\u001B[39m]))\n\u001B[0;32m    441\u001B[0m     chained_exceptions_tb_offset \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    442\u001B[0m     out_list \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 443\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m            \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchained_exc_ids\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchained_exceptions_tb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m         \u001B[38;5;241m+\u001B[39m chained_exception_message\n\u001B[0;32m    447\u001B[0m         \u001B[38;5;241m+\u001B[39m out_list)\n\u001B[0;32m    449\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out_list\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1118\u001B[0m, in \u001B[0;36mAutoFormattedTB.structured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1116\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1117\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtb \u001B[38;5;241m=\u001B[39m tb\n\u001B[1;32m-> 1118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFormattedTB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1119\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_lines_of_context\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1012\u001B[0m, in \u001B[0;36mFormattedTB.structured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1009\u001B[0m mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode\n\u001B[0;32m   1010\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose_modes:\n\u001B[0;32m   1011\u001B[0m     \u001B[38;5;66;03m# Verbose modes need a full traceback\u001B[39;00m\n\u001B[1;32m-> 1012\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVerboseTB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_lines_of_context\u001B[49m\n\u001B[0;32m   1014\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMinimal\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ListTB\u001B[38;5;241m.\u001B[39mget_exception_only(\u001B[38;5;28mself\u001B[39m, etype, value)\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:865\u001B[0m, in \u001B[0;36mVerboseTB.structured_traceback\u001B[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m    856\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstructured_traceback\u001B[39m(\n\u001B[0;32m    857\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    858\u001B[0m     etype: \u001B[38;5;28mtype\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    862\u001B[0m     number_of_lines_of_context: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m,\n\u001B[0;32m    863\u001B[0m ):\n\u001B[0;32m    864\u001B[0m     \u001B[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 865\u001B[0m     formatted_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_exception_as_a_whole\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_lines_of_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m                                                           \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    868\u001B[0m     colors \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mColors  \u001B[38;5;66;03m# just a shorthand + quicker name lookup\u001B[39;00m\n\u001B[0;32m    869\u001B[0m     colorsnormal \u001B[38;5;241m=\u001B[39m colors\u001B[38;5;241m.\u001B[39mNormal  \u001B[38;5;66;03m# used a lot\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:799\u001B[0m, in \u001B[0;36mVerboseTB.format_exception_as_a_whole\u001B[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[0;32m    796\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tb_offset, \u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m    797\u001B[0m head \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_header(etype, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlong_header)\n\u001B[0;32m    798\u001B[0m records \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 799\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_records\u001B[49m\u001B[43m(\u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_lines_of_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m etb \u001B[38;5;28;01melse\u001B[39;00m []\n\u001B[0;32m    800\u001B[0m )\n\u001B[0;32m    802\u001B[0m frames \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    803\u001B[0m skipped \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:854\u001B[0m, in \u001B[0;36mVerboseTB.get_records\u001B[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[0;32m    848\u001B[0m     formatter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    849\u001B[0m options \u001B[38;5;241m=\u001B[39m stack_data\u001B[38;5;241m.\u001B[39mOptions(\n\u001B[0;32m    850\u001B[0m     before\u001B[38;5;241m=\u001B[39mbefore,\n\u001B[0;32m    851\u001B[0m     after\u001B[38;5;241m=\u001B[39mafter,\n\u001B[0;32m    852\u001B[0m     pygments_formatter\u001B[38;5;241m=\u001B[39mformatter,\n\u001B[0;32m    853\u001B[0m )\n\u001B[1;32m--> 854\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstack_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFrameInfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[tb_offset:]\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\stack_data\\core.py:546\u001B[0m, in \u001B[0;36mFrameInfo.stack_data\u001B[1;34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001B[0m\n\u001B[0;32m    530\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstack_data\u001B[39m(\n\u001B[0;32m    532\u001B[0m         \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    536\u001B[0m         collapse_repeated_frames: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    537\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Union[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFrameInfo\u001B[39m\u001B[38;5;124m'\u001B[39m, RepeatedFrames]]:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;124;03m    An iterator of FrameInfo and RepeatedFrames objects representing\u001B[39;00m\n\u001B[0;32m    540\u001B[0m \u001B[38;5;124;03m    a full traceback or stack. Similar consecutive frames are collapsed into RepeatedFrames\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;124;03m    and optionally an Options object to configure.\u001B[39;00m\n\u001B[0;32m    545\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 546\u001B[0m     stack \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miter_stack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe_or_tb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    548\u001B[0m     \u001B[38;5;66;03m# Reverse the stack from a frame so that it's in the same order\u001B[39;00m\n\u001B[0;32m    549\u001B[0m     \u001B[38;5;66;03m# as the order from a traceback, which is the order of a printed\u001B[39;00m\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;66;03m# traceback when read top to bottom (most recent call last)\u001B[39;00m\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_frame(frame_or_tb):\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\stack_data\\utils.py:98\u001B[0m, in \u001B[0;36miter_stack\u001B[1;34m(frame_or_tb)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m frame_or_tb:\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m frame_or_tb\n\u001B[1;32m---> 98\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mis_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe_or_tb\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     99\u001B[0m         frame_or_tb \u001B[38;5;241m=\u001B[39m frame_or_tb\u001B[38;5;241m.\u001B[39mf_back\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\stack_data\\utils.py:91\u001B[0m, in \u001B[0;36mis_frame\u001B[1;34m(frame_or_tb)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_frame\u001B[39m(frame_or_tb: Union[FrameType, TracebackType]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m---> 91\u001B[0m     \u001B[43massert_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mframe_or_tb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFrameType\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTracebackType\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(frame_or_tb, (types\u001B[38;5;241m.\u001B[39mFrameType,))\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\stack_data\\utils.py:172\u001B[0m, in \u001B[0;36massert_\u001B[1;34m(condition, error)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    171\u001B[0m     error \u001B[38;5;241m=\u001B[39m \u001B[38;5;167;01mAssertionError\u001B[39;00m(error)\n\u001B[1;32m--> 172\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m error\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "already_connected_mariadb = False\n",
    "# Connect to MariaDB Platform\n",
    "if not already_connected_mariadb:\n",
    "    try:\n",
    "        conn = mariadb.connect(\n",
    "            user=\"root\",\n",
    "            password=\"armando12\",\n",
    "            host=\"localhost\",\n",
    "            port=3306,\n",
    "            database=\"maadb_tweets\"\n",
    "        )\n",
    "    except mariadb.Error as e:\n",
    "        print(f\"Error connecting to MariaDB Platform: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Get Cursor\n",
    "    cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SHOW TABLES\")\n",
    "\n",
    "for (table_name,) in cur:\n",
    "    print(table_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db_tweets_collection = mydb[\"Tweets\"]\n",
    "#\n",
    "# pipeline = [\n",
    "#     # First Stage\n",
    "#     {\n",
    "#         \"$group\" :\n",
    "#             {\n",
    "#                 \"_id\" : \"$item\",\n",
    "#                 \"sum_of_something\": { \"$sum\": { \"$multiply\": [ \"$price\", \"$quantity\" ] } }\n",
    "#             }\n",
    "#     },\n",
    "#\n",
    "#     # Second Stage\n",
    "#     {\n",
    "#        \"$match\": { \"sum_of_something\": { \"$gte\": 100 } }\n",
    "#     }\n",
    "# ]\n",
    "#\n",
    "# mydb.db_tweets_collection.aggregate(pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}