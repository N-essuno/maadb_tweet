{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gianl\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Gianl\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\Gianl\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from src.LexicalResource import LexicalResource\n",
    "from src.Tweet import TweetInfo, Tweet, LEX_RESOURCES_DIRECTORY, TWEETS_DIRECTORY\n",
    "# Se ne abbiamo voglia possiamo mettere le emoticons e gli emoji su file e per poi leggerli\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "import pymongo\n",
    "\n",
    "from time import perf_counter\n",
    "from datetime import timedelta\n",
    "\n",
    "ALLOW_PRINT = False\n",
    "N_TWEET_TO_READ = 40000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Read files in directory\n",
    "General function to read text files from a directory and merge them"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "lex_resources_list: List[LexicalResource] = []\n",
    "\n",
    "def read_texts_in_directory(directory_path: str, sentiment: str) -> List[str]:\n",
    "    files_text_list: List[str] = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            file = open(file=file_path, encoding=\"utf8\")\n",
    "            file_text = file.read().split() # list of words for a single lex resource of a sentiment\n",
    "            files_text_list = files_text_list + file_text\n",
    "\n",
    "            lex_res: LexicalResource = LexicalResource(filename, sentiment)\n",
    "            lex_res.add_word_list(file_text)\n",
    "            global lex_resources_list\n",
    "            lex_resources_list.append(lex_res)\n",
    "\n",
    "    # print(len(lex_resources_list))\n",
    "    # [print(i) for i in lex_resources_list]\n",
    "    return files_text_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Read lexical resources for a sentiment\n",
    "Function which reads all the lexical resources for a sentiment\n",
    "The directory containing all lexical resources for that sentiment is passed as parameter\n",
    "Returns a set of the words in all the lexical resources of a sentiment\n",
    "### forse creare per ogni lex res di OGNI sentimento un dizionario diverso? Bisogna vedere come caricare i dati su db, bisogna caricare ogni lex res diversa di ogni sentimento sul db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def read_lex_resources_sentiment(sentiment_lex_resources_directory: str, sentiment: str) -> Set[str]:\n",
    "    resource_words: Set[str] = set()\n",
    "    resources_text: List[str] = read_texts_in_directory(sentiment_lex_resources_directory, sentiment)\n",
    "    for word in resources_text:\n",
    "        if not '_' in word:\n",
    "            resource_words.add(word)\n",
    "    #print(sentiment, \"\\n\", resource_words, \"\\n\\n\")\n",
    "    return resource_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read all lexical resources\n",
    "\n",
    "Reads all the lexical resources and returns a dictionary of word to sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "time_lex_res_start = perf_counter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "sentiment_lex_resources: Dict[str, str] = {}\n",
    "\n",
    "for resources_path, sentiments, _ in os.walk(LEX_RESOURCES_DIRECTORY):\n",
    "    # The folders inside the lexical resources folder are named after a sentiment (Ex. Anger, Joy), each of them contain some files and each of them is a list of words that are associated with that sentiment\n",
    "    for sentiment in sentiments:\n",
    "        # iterate each folder (one for sentiment)\n",
    "        resources_sentiment_path = os.path.join(resources_path, sentiment)\n",
    "        sentiment_words_set: Set[str] = read_lex_resources_sentiment(resources_sentiment_path, sentiment)\n",
    "\n",
    "        # read the files containing lists of words, and return a set of all the words in those files\n",
    "        for sentiment_word in sentiment_words_set:\n",
    "            # associate each word of the set to the corresponding sentiment\n",
    "            sentiment_lex_resources[sentiment_word] = sentiment\n",
    "\n",
    "lex_word_to_sentiment = sentiment_lex_resources\n",
    "if ALLOW_PRINT:\n",
    "    print(lex_word_to_sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for loading lexical resources:  0:00:00.051487\n"
     ]
    }
   ],
   "source": [
    "time_lex_res_end = perf_counter()\n",
    "time_lex_res = time_lex_res_end - time_lex_res_start\n",
    "print(\"Elapsed time for loading lexical resources: \", str(timedelta(seconds=time_lex_res)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tweet reading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Reads a file and converts the text to tweets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def read_tweet_file(file_path_string: str, sentiment: str, n_tweet_to_read: int) -> List[Tweet]:\n",
    "    \"\"\"\n",
    "    Reads a file and converts the text to tweets\n",
    "    :param file_path_string: string of the path to the file\n",
    "    \"\"\"\n",
    "\n",
    "    # tweets read from file\n",
    "    tweets_read: List[Tweet] = []\n",
    "\n",
    "    tweets_file = open(file=file_path_string, encoding=\"utf8\")\n",
    "    tweets_text: List[str] = tweets_file.readlines()\n",
    "\n",
    "    # For each tweet text create a Tweet object\n",
    "    if n_tweet_to_read > len(tweets_text):\n",
    "        n_tweet_to_read = len(tweets_text)\n",
    "    for tweet_index in range(0, n_tweet_to_read):\n",
    "        new_tweet = Tweet(tweets_text[tweet_index], tweet_index+1, sentiment)\n",
    "        tweets_read.append(new_tweet)\n",
    "\n",
    "    return tweets_read"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get list of sentiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sentiments: List[str] = [sentiment for sentiment in os.listdir(LEX_RESOURCES_DIRECTORY)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read tweets folder and load Tweet Info for stem counting\n",
    "The tweets folder contains for each sentiment a file containing tweets of that sentiment. Each file is scanned and for each tweet a TweetInfo object is created in order to maintain the count of how many word of which sentiments are in it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_tweet_sentiment_from_file_name(file_name: str):\n",
    "    extension_removed = file_name.split(\".\")[0]\n",
    "    sentiment = extension_removed.split(\"_\")[-2]\n",
    "    return sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "time_tweets_start = perf_counter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "tweets_to_info: Dict[Tweet, TweetInfo] = {}\n",
    "for tweets_sentiments_directory, _, tweets_sentiments_filenames in os.walk(TWEETS_DIRECTORY):\n",
    "\n",
    "    for tweets_sentiment_filename in tweets_sentiments_filenames:\n",
    "        tweets_sentiment_filepath = os.path.join(tweets_sentiments_directory, tweets_sentiment_filename)\n",
    "        sentiment = get_tweet_sentiment_from_file_name(tweets_sentiment_filename)\n",
    "        tweets_for_sentiment: List[Tweet] = read_tweet_file(tweets_sentiment_filepath, sentiment, N_TWEET_TO_READ)\n",
    "        for tweet in tweets_for_sentiment:\n",
    "            tweet_info: TweetInfo = TweetInfo(sentiment, sentiments)\n",
    "            tweet.tweet_stem_count = TweetInfo\n",
    "            tweets_to_info[tweet] = tweet_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for loading tweets:  0:08:43.479967\n"
     ]
    }
   ],
   "source": [
    "time_tweets_end = perf_counter()\n",
    "time_tweets = time_tweets_end - time_tweets_start\n",
    "print(\"Elapsed time for loading tweets: \", str(timedelta(seconds=time_tweets)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stem counting\n",
    "For each tweet and each word of them is checked the sentiment and increased the counter for that sentiment in the TweetInfo object associated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "for tweet in tweets_to_info:\n",
    "    tweet_info = tweets_to_info[tweet]\n",
    "    tweet_words: List[str] = tweet.get_words()\n",
    "\n",
    "    for word in tweet_words:\n",
    "        if word in lex_word_to_sentiment:\n",
    "            # get the sentiment for the word and increase sentiment counter by 1\n",
    "            sentiment = lex_word_to_sentiment[word]\n",
    "            tweet_info.increase_sentiment_counter(sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test print"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def print_tweets():\n",
    "    for tweet in tweets_to_info.keys():\n",
    "        info = tweets_to_info[tweet]\n",
    "        print(tweet)\n",
    "        print(\"sentiment: \" + info.sentiment)\n",
    "        print(\"sentiment occurrences: \")\n",
    "        print(info.sentiment_occurrences)\n",
    "        print(\"---\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "if ALLOW_PRINT:\n",
    "    print_tweets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MongoDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "already_connected = False\n",
    "if not already_connected:\n",
    "    mongo_client = pymongo.MongoClient(\"mongodb+srv://Peppino:wHzr79JxnRUgK52@cluster0.zkagq.mongodb.net/?retryWrites=true&w=majority\")\n",
    "    mydb = mongo_client[\"maadb_tweets\"]\n",
    "\n",
    "coll_list = mydb.list_collection_names()\n",
    "\n",
    "if ALLOW_PRINT:\n",
    "    [print(i) for i in lex_resources_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check in which resources each word is contained\n",
    "Creates a dictionary <word, lex_res_list> to map each word with the lexical resources which contains the word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "map_word_lex_res: Dict[str, List[str]] = {}\n",
    "\n",
    "for word in lex_word_to_sentiment:\n",
    "    for lex_res in lex_resources_list:\n",
    "        if word in lex_res.word_list:\n",
    "            if map_word_lex_res.get(word) is None:\n",
    "                map_word_lex_res[word] = [lex_res.filename]\n",
    "            else:\n",
    "                map_word_lex_res[word].append(lex_res.filename)\n",
    "\n",
    "if ALLOW_PRINT:\n",
    "    print(map_word_lex_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flags to manage queries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "delete_lex_res = True\n",
    "insert_lex_res = True\n",
    "\n",
    "delete_lex_res_words = True\n",
    "insert_lex_res_words = True\n",
    "\n",
    "delete_tweets = True\n",
    "insert_tweets = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete Lexical Resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "db_lex_res_collection = mydb[\"LexResources\"]\n",
    "lex_res_list_to_upload = []\n",
    "time_lex_res_mongo_start = 0\n",
    "\n",
    "if delete_lex_res:\n",
    "    db_lex_res_collection.delete_many({})\n",
    "\n",
    "if insert_lex_res:\n",
    "    for lex_res in lex_resources_list:\n",
    "        to_upload = {\"_id\" : lex_res.filename,\n",
    "                     \"sentiment\":  lex_res.sentiment.lower(),\n",
    "                     \"totNumberWords\" : lex_res.get_number_of_words()}\n",
    "        lex_res_list_to_upload.append(to_upload)\n",
    "\n",
    "    time_lex_res_mongo_start = perf_counter()\n",
    "    inserted_lex_res = db_lex_res_collection.insert_many(lex_res_list_to_upload)\n",
    "    if ALLOW_PRINT:\n",
    "        print(lex_res_list_to_upload)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for inserting lexical resources in mongo:  0:00:00.044006\n"
     ]
    }
   ],
   "source": [
    "time_lex_res_mongo_end = perf_counter()\n",
    "time_lex_res_mongo = time_lex_res_mongo_end - time_lex_res_mongo_start\n",
    "print(\"Elapsed time for inserting lexical resources in mongo: \", str(timedelta(seconds=time_lex_res_mongo)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete words of lexical resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "time_lex_res_words_mongo_start = perf_counter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "map_lex_word_db_id: Dict[str, int] = {}\n",
    "db_lex_res_words_collection = mydb[\"LexResourcesWords\"]\n",
    "\n",
    "\n",
    "if delete_lex_res_words:\n",
    "    db_lex_res_words_collection.delete_many({})\n",
    "\n",
    "if insert_lex_res_words:\n",
    "    # for each word in all the lexical resources insert in LexResWords the word and a\n",
    "    # list of pairs <$ref, $id> to track in which LexRes the word is contained\n",
    "    for word in lex_word_to_sentiment:\n",
    "        list_lex_res = map_word_lex_res[word] # list of lexical resources in which the word is contained\n",
    "        resources = [] # list of pairs to insert in LexResWords\n",
    "\n",
    "        for res in list_lex_res: # populate list adding, one at a time, the lexical resources in which the word is contained\n",
    "            resources.append({\"$ref\": \"LexResources\", \"$id\": res})\n",
    "\n",
    "        word_to_upload = {\"lemma\" : word,\n",
    "                          \"resources\" : resources}\n",
    "        inserted_lex_res_word = db_lex_res_words_collection.insert_one(word_to_upload)\n",
    "        map_lex_word_db_id[word] = inserted_lex_res_word.inserted_id # save object id to use it later to reference resources words from tweet words\n",
    "        if ALLOW_PRINT:\n",
    "            print(word_to_upload)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for inserting lexical resources words in mongo:  0:02:44.617414\n"
     ]
    }
   ],
   "source": [
    "time_lex_res_words_mongo_end = perf_counter()\n",
    "time_lex_res_words_mongo = time_lex_res_words_mongo_end - time_lex_res_words_mongo_start\n",
    "print(\"Elapsed time for inserting lexical resources words in mongo: \", str(timedelta(seconds=time_lex_res_words_mongo)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "db_tweets_collection = mydb[\"Tweets\"]\n",
    "tweet_list_to_upload = []\n",
    "time_tweets_mongo_start = 0\n",
    "\n",
    "if delete_tweets:\n",
    "    db_tweets_collection.delete_many({})\n",
    "\n",
    "if insert_tweets:\n",
    "    for tweet in tweets_to_info:\n",
    "        tweet_words_upload = []\n",
    "        for word in tweet.pos_tags:\n",
    "            if map_lex_word_db_id.get(word) is None:\n",
    "                # print(map_lex_word_db_id.get(word))\n",
    "                # print(tweet)\n",
    "                # Decide what to do with words that do not have a lexical resource associated, we could think about associating it to a resource or some other strategy.\n",
    "                tweet_words_upload.append({\n",
    "                    \"lemma\": word,\n",
    "                    \"POS\": tweet.pos_tags[word],\n",
    "                    \"freq\": tweet.word_frequency[word],\n",
    "                    \"in_lex_resources\" : \"None\"})\n",
    "            else:\n",
    "                tweet_words_upload.append({\n",
    "                    \"lemma\": word,\n",
    "                    \"POS\": tweet.pos_tags[word],\n",
    "                    \"freq\": tweet.word_frequency[word],\n",
    "                    \"in_lex_resources\" : {\"$ref\": \"LexResourcesWords\", \"$id\": map_lex_word_db_id[word]}})\n",
    "\n",
    "        tweet_to_upload = {\n",
    "            \"sentiment\": tweet.sentiment.lower(),\n",
    "            \"index\": tweet.index,\n",
    "            \"words\" : tweet_words_upload,\n",
    "            \"hashtags\" : tweet.hashtags,\n",
    "            \"emojis\" : tweet.emojis,\n",
    "            \"emoticons\" : tweet.emoticons}\n",
    "\n",
    "        tweet_list_to_upload.append(tweet_to_upload)\n",
    "\n",
    "    time_tweets_mongo_start = perf_counter()\n",
    "    inserted_tweets = db_tweets_collection.insert_many(tweet_list_to_upload)\n",
    "    if ALLOW_PRINT:\n",
    "        print(tweet_list_to_upload)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for inserting tweets in mongo:  0:00:55.862451\n"
     ]
    }
   ],
   "source": [
    "time_tweets_mongo_end = perf_counter()\n",
    "time_tweets_mongo = time_tweets_mongo_end - time_tweets_mongo_start\n",
    "print(\"Elapsed time for inserting tweets in mongo: \", str(timedelta(seconds=time_tweets_mongo)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline for goal 1 (word cloud)\n",
    "\n",
    "For tweets of sentiment S obtains the frequency of each word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "OperationFailure",
     "evalue": "PlanExecutor error during aggregation :: caused by :: Exceeded memory limit for $group, but didn't allow external sort. Pass allowDiskUse:true to opt in., full error: {'ok': 0.0, 'errmsg': \"PlanExecutor error during aggregation :: caused by :: Exceeded memory limit for $group, but didn't allow external sort. Pass allowDiskUse:true to opt in.\", 'code': 292, 'codeName': 'QueryExceededMemoryLimitNoDiskUseAllowed', '$clusterTime': {'clusterTime': Timestamp(1656974339, 12), 'signature': {'hash': b'nl\\xb6\\xf3\\xbe~\\x90\\xce\\xf1\\xf8ieZz+n\\\\`Y\\xfa', 'keyId': 7080873913877528583}}, 'operationTime': Timestamp(1656974339, 12)}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperationFailure\u001B[0m                          Traceback (most recent call last)",
      "Input \u001B[1;32mIn [27]\u001B[0m, in \u001B[0;36m<cell line: 223>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m wc_pipeline \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      4\u001B[0m     {\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$group\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    219\u001B[0m     }\n\u001B[0;32m    220\u001B[0m ]\n\u001B[0;32m    222\u001B[0m time_pip1_start \u001B[38;5;241m=\u001B[39m perf_counter()\n\u001B[1;32m--> 223\u001B[0m wc_pip_result \u001B[38;5;241m=\u001B[39m \u001B[43mdb_tweets_collection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maggregate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwc_pipeline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowDiskUse\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\pymongo\\collection.py:2402\u001B[0m, in \u001B[0;36mCollection.aggregate\u001B[1;34m(self, pipeline, session, let, comment, **kwargs)\u001B[0m\n\u001B[0;32m   2324\u001B[0m \u001B[38;5;124;03m\"\"\"Perform an aggregation using the aggregation framework on this\u001B[39;00m\n\u001B[0;32m   2325\u001B[0m \u001B[38;5;124;03mcollection.\u001B[39;00m\n\u001B[0;32m   2326\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2398\u001B[0m \u001B[38;5;124;03m    https://mongodb.com/docs/manual/reference/command/aggregate\u001B[39;00m\n\u001B[0;32m   2399\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2401\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__database\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_tmp_session(session, close\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m s:\n\u001B[1;32m-> 2402\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_aggregate(\n\u001B[0;32m   2403\u001B[0m         _CollectionAggregationCommand,\n\u001B[0;32m   2404\u001B[0m         pipeline,\n\u001B[0;32m   2405\u001B[0m         CommandCursor,\n\u001B[0;32m   2406\u001B[0m         session\u001B[38;5;241m=\u001B[39ms,\n\u001B[0;32m   2407\u001B[0m         explicit_session\u001B[38;5;241m=\u001B[39msession \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2408\u001B[0m         let\u001B[38;5;241m=\u001B[39mlet,\n\u001B[0;32m   2409\u001B[0m         comment\u001B[38;5;241m=\u001B[39mcomment,\n\u001B[0;32m   2410\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2411\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\pymongo\\collection.py:2309\u001B[0m, in \u001B[0;36mCollection._aggregate\u001B[1;34m(self, aggregation_command, pipeline, cursor_class, session, explicit_session, let, comment, **kwargs)\u001B[0m\n\u001B[0;32m   2298\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomment\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m comment\n\u001B[0;32m   2299\u001B[0m cmd \u001B[38;5;241m=\u001B[39m aggregation_command(\n\u001B[0;32m   2300\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2301\u001B[0m     cursor_class,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2306\u001B[0m     user_fields\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcursor\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirstBatch\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m}},\n\u001B[0;32m   2307\u001B[0m )\n\u001B[1;32m-> 2309\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__database\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retryable_read\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2310\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcmd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cursor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2311\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcmd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_read_preference\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2312\u001B[0m \u001B[43m    \u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretryable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcmd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_performs_write\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2314\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\pymongo\\mongo_client.py:1371\u001B[0m, in \u001B[0;36mMongoClient._retryable_read\u001B[1;34m(self, func, read_pref, session, address, retryable)\u001B[0m\n\u001B[0;32m   1369\u001B[0m             \u001B[38;5;28;01massert\u001B[39;00m last_error \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1370\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m last_error\n\u001B[1;32m-> 1371\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserver\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msock_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread_pref\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ServerSelectionTimeoutError:\n\u001B[0;32m   1373\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m retrying:\n\u001B[0;32m   1374\u001B[0m         \u001B[38;5;66;03m# The application may think the write was never attempted\u001B[39;00m\n\u001B[0;32m   1375\u001B[0m         \u001B[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001B[39;00m\n\u001B[0;32m   1376\u001B[0m         \u001B[38;5;66;03m# attempt. Raise the original exception instead.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\pymongo\\aggregation.py:140\u001B[0m, in \u001B[0;36m_AggregationCommand.get_cursor\u001B[1;34m(self, session, server, sock_info, read_preference)\u001B[0m\n\u001B[0;32m    137\u001B[0m     write_concern \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;66;03m# Run command.\u001B[39;00m\n\u001B[1;32m--> 140\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43msock_info\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommand\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_database\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcmd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m    \u001B[49m\u001B[43mread_preference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_target\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcodec_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparse_write_concern_error\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mread_concern\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread_concern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwrite_concern\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwrite_concern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_collation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_database\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m    \u001B[49m\u001B[43muser_fields\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_user_fields\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_processor:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_processor(result, sock_info)\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\pymongo\\pool.py:743\u001B[0m, in \u001B[0;36mSocketInfo.command\u001B[1;34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001B[0m\n\u001B[0;32m    741\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_if_not_writable(unacknowledged)\n\u001B[0;32m    742\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcommand\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    744\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdbname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m        \u001B[49m\u001B[43mspec\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    747\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_mongos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    748\u001B[0m \u001B[43m        \u001B[49m\u001B[43mread_preference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    749\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcodec_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    750\u001B[0m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    751\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    752\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    753\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallowable_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    754\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddress\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    755\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlisteners\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    756\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_bson_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    757\u001B[0m \u001B[43m        \u001B[49m\u001B[43mread_concern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    758\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_write_concern_error\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_write_concern_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    759\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcollation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    760\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompression_ctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompression_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    761\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_op_msg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mop_msg_enabled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    762\u001B[0m \u001B[43m        \u001B[49m\u001B[43munacknowledged\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munacknowledged\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    763\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_fields\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_fields\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    764\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexhaust_allowed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexhaust_allowed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    765\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    766\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (OperationFailure, NotPrimaryError):\n\u001B[0;32m    767\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\pymongo\\network.py:160\u001B[0m, in \u001B[0;36mcommand\u001B[1;34m(sock_info, dbname, spec, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed)\u001B[0m\n\u001B[0;32m    158\u001B[0m             client\u001B[38;5;241m.\u001B[39m_process_response(response_doc, session)\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m check:\n\u001B[1;32m--> 160\u001B[0m             \u001B[43mhelpers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_command_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m                \u001B[49m\u001B[43mresponse_doc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[43m                \u001B[49m\u001B[43msock_info\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_wire_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[43m                \u001B[49m\u001B[43mallowable_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m                \u001B[49m\u001B[43mparse_write_concern_error\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_write_concern_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m publish:\n",
      "File \u001B[1;32m~\\PycharmProjects\\maadb_tweet\\venv\\lib\\site-packages\\pymongo\\helpers.py:180\u001B[0m, in \u001B[0;36m_check_command_response\u001B[1;34m(response, max_wire_version, allowable_errors, parse_write_concern_error)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m43\u001B[39m:\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CursorNotFound(errmsg, code, response, max_wire_version)\n\u001B[1;32m--> 180\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m OperationFailure(errmsg, code, response, max_wire_version)\n",
      "\u001B[1;31mOperationFailure\u001B[0m: PlanExecutor error during aggregation :: caused by :: Exceeded memory limit for $group, but didn't allow external sort. Pass allowDiskUse:true to opt in., full error: {'ok': 0.0, 'errmsg': \"PlanExecutor error during aggregation :: caused by :: Exceeded memory limit for $group, but didn't allow external sort. Pass allowDiskUse:true to opt in.\", 'code': 292, 'codeName': 'QueryExceededMemoryLimitNoDiskUseAllowed', '$clusterTime': {'clusterTime': Timestamp(1656974339, 12), 'signature': {'hash': b'nl\\xb6\\xf3\\xbe~\\x90\\xce\\xf1\\xf8ieZz+n\\\\`Y\\xfa', 'keyId': 7080873913877528583}}, 'operationTime': Timestamp(1656974339, 12)}"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "wc_pipeline = [\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': '$sentiment',\n",
    "            'emoticons': {\n",
    "                '$push': '$emoticons'\n",
    "            },\n",
    "            'emojis': {\n",
    "                '$push': '$emojis'\n",
    "            },\n",
    "            'words': {\n",
    "                '$push': '$words'\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$addFields': {\n",
    "            'words': {\n",
    "                '$reduce': {\n",
    "                    'input': '$words',\n",
    "                    'initialValue': [],\n",
    "                    'in': {\n",
    "                        '$concatArrays': [\n",
    "                            '$$this', '$$value'\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'emoticons': {\n",
    "                '$reduce': {\n",
    "                    'input': '$emoticons',\n",
    "                    'initialValue': [],\n",
    "                    'in': {\n",
    "                        '$concatArrays': [\n",
    "                            '$$value', '$$this'\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'emojis': {\n",
    "                '$reduce': {\n",
    "                    'input': '$emojis',\n",
    "                    'initialValue': [],\n",
    "                    'in': {\n",
    "                        '$concatArrays': [\n",
    "                            '$$value', '$$this'\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$addFields': {\n",
    "            'wordsProportion': {\n",
    "                '$map': {\n",
    "                    'input': '$words',\n",
    "                    'as': 'word',\n",
    "                    'in': {\n",
    "                        'lemma': '$$word.lemma',\n",
    "                        'proportion': {\n",
    "                            '$reduce': {\n",
    "                                'input': '$words',\n",
    "                                'initialValue': 0,\n",
    "                                'in': {\n",
    "                                    '$cond': [\n",
    "                                        {\n",
    "                                            '$eq': [\n",
    "                                                '$$this.lemma', '$$word.lemma'\n",
    "                                            ]\n",
    "                                        }, {\n",
    "                                            '$add': [\n",
    "                                                '$$value', '$$this.freq'\n",
    "                                            ]\n",
    "                                        }, {\n",
    "                                            '$add': [\n",
    "                                                '$$value', 0\n",
    "                                            ]\n",
    "                                        }\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'emoticonsProportion': {\n",
    "                '$map': {\n",
    "                    'input': {\n",
    "                        '$setUnion': '$emoticons'\n",
    "                    },\n",
    "                    'as': 'emoticon',\n",
    "                    'in': {\n",
    "                        'name': '$$emoticon',\n",
    "                        'proportion': {\n",
    "                            '$size': {\n",
    "                                '$filter': {\n",
    "                                    'input': '$emoticons',\n",
    "                                    'cond': {\n",
    "                                        '$eq': [\n",
    "                                            '$$this', '$$emoticon'\n",
    "                                        ]\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'emojisProportion': {\n",
    "                '$map': {\n",
    "                    'input': {\n",
    "                        '$setUnion': '$emojis'\n",
    "                    },\n",
    "                    'as': 'emoji',\n",
    "                    'in': {\n",
    "                        'name': '$$emoji',\n",
    "                        'proportion': {\n",
    "                            '$size': {\n",
    "                                '$filter': {\n",
    "                                    'input': '$emojis',\n",
    "                                    'cond': {\n",
    "                                        '$eq': [\n",
    "                                            '$$this', '$$emoji'\n",
    "                                        ]\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            'emoticons': 1,\n",
    "            'emojis': 1,\n",
    "            'words': 1,\n",
    "            'wordsProportion': {\n",
    "                '$setUnion': [\n",
    "                    '$wordsProportion', []\n",
    "                ]\n",
    "            },\n",
    "            'emojisProportion': 1,\n",
    "            'emoticonsProportion': 1\n",
    "        }\n",
    "    }, {\n",
    "        '$addFields': {\n",
    "            'numWords': {\n",
    "                '$sum': '$wordsProportion.proportion'\n",
    "            },\n",
    "            'numEmojis': {\n",
    "                '$size': '$emojis'\n",
    "            },\n",
    "            'numEmoticons': {\n",
    "                '$size': '$emoticons'\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$addFields': {\n",
    "            'numTokens': {\n",
    "                '$add': [\n",
    "                    '$numEmoticons', '$numWords', '$numEmojis'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$set': {\n",
    "            'wordsProportion': {\n",
    "                '$map': {\n",
    "                    'input': '$wordsProportion',\n",
    "                    'as': 'word',\n",
    "                    'in': {\n",
    "                        'name': '$$word.lemma',\n",
    "                        'proportion': {\n",
    "                            '$divide': [\n",
    "                                '$$word.proportion', '$numTokens'\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'emojisProportion': {\n",
    "                '$map': {\n",
    "                    'input': '$emojisProportion',\n",
    "                    'as': 'emoji',\n",
    "                    'in': {\n",
    "                        'name': '$$emoji.name',\n",
    "                        'proportion': {\n",
    "                            '$divide': [\n",
    "                                '$$emoji.proportion', '$numTokens'\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            'emoticonsProportion': {\n",
    "                '$map': {\n",
    "                    'input': '$emoticonsProportion',\n",
    "                    'as': 'emoticon',\n",
    "                    'in': {\n",
    "                        'name': '$$emoticon.name',\n",
    "                        'proportion': {\n",
    "                            '$divide': [\n",
    "                                '$$emoticon.proportion', '$numTokens'\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            '_id': 1,\n",
    "            'wordsProportion': 1,\n",
    "            'emojisProportion': 1,\n",
    "            'emoticonsProportion': 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "time_pip1_start = perf_counter()\n",
    "wc_pip_result = db_tweets_collection.aggregate(wc_pipeline, allowDiskUse = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_pip1_end = perf_counter()\n",
    "time_pip1 = time_pip1_end - time_pip1_start\n",
    "print(\"Elapsed time for pipeline 1 (word clouds): \", str(timedelta(seconds=time_pip1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_pip1_res = list(wc_pip_result)\n",
    "if ALLOW_PRINT:\n",
    "    pprint(list_pip1_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wordclouds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words_proportions = {}\n",
    "emojis_proportions = {}\n",
    "emoticons_proportions = {}\n",
    "for res in list_pip1_res:\n",
    "    single_words_dict = {}\n",
    "    for dic in res[\"wordsProportion\"]: # merge list of dictionaries {word, frequency} into a single dictionary\n",
    "        single_words_dict[dic[\"name\"]] = dic[\"proportion\"]\n",
    "    words_proportions[res[\"_id\"]] = single_words_dict\n",
    "\n",
    "    single_emojis_dict = {}\n",
    "    for dic in res[\"emojisProportion\"]:\n",
    "        single_emojis_dict[dic[\"name\"]] = dic[\"proportion\"]\n",
    "    emojis_proportions[res[\"_id\"]] = single_emojis_dict\n",
    "\n",
    "    single_emoticons_dict = {}\n",
    "    for dic in res[\"emoticonsProportion\"]: #\n",
    "        single_emoticons_dict[dic[\"name\"]] = dic[\"proportion\"]\n",
    "    emoticons_proportions[res[\"_id\"]] = single_emoticons_dict\n",
    "\n",
    "if ALLOW_PRINT:\n",
    "    print(words_proportions)\n",
    "    print(emojis_proportions)\n",
    "    print(emoticons_proportions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "# the regex used to detect words is a combination of normal words, ascii art, and emojis\n",
    "# 2+ consecutive letters (also include apostrophes), e.x It's\n",
    "normal_word = r\"(?:\\w[\\w']+)\"\n",
    "# 2+ consecutive punctuations, e.x. :)\n",
    "ascii_art = r\"(?:[{punctuation}][{punctuation}]+)\".format(punctuation=string.punctuation)\n",
    "# a single character that is not alpha_numeric or other ascii printable\n",
    "emoji = r\"(?:[^\\s])(?<![\\w{ascii_printable}])\".format(ascii_printable=string.printable)\n",
    "regexp = r\"{normal_word}|{ascii_art}|{emoji}\".format(normal_word=normal_word, ascii_art=ascii_art,\n",
    "                                                     emoji=emoji)\n",
    "\n",
    "# Generate a word cloud image\n",
    "# The Symbola font includes most emoji\n",
    "font_path = 'resources/Symbola.otf'\n",
    "\n",
    "n_elem_to_plot = -1\n",
    "\n",
    "for sentiment in words_proportions: # Word cloud\n",
    "    if words_proportions.get(sentiment) is not None and len(words_proportions[sentiment]) > 0:\n",
    "        words_to_check = words_proportions[sentiment]\n",
    "        if 0 < n_elem_to_plot <= len(words_proportions[sentiment]):\n",
    "            words_to_check = dict(sorted(words_to_check.items(), key=lambda x: x[1], reverse=True)) # sort by frequency\n",
    "            words_to_check = dict(list(words_to_check.items())[:n_elem_to_plot]) #take first n_elem_to_plot items from dictionary\n",
    "\n",
    "        wordcloud = WordCloud(width=500, height=500,\n",
    "                              background_color='white',\n",
    "                              min_font_size=10,\n",
    "                              font_path=font_path,\n",
    "                              regexp=regexp).generate_from_frequencies(words_to_check)\n",
    "\n",
    "        # plot the WordCloud image\n",
    "        plt.figure(figsize=(5, 5), facecolor=None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.title(\"word cloud for sentiment: \" + sentiment)\n",
    "        plt.savefig('./resources/ouput_img/wc_words_' + sentiment)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for sentiment in emojis_proportions: # Emojis cloud\n",
    "    if emojis_proportions.get(sentiment) is not None and len(emojis_proportions[sentiment]) > 0:\n",
    "        emojis_to_check = emojis_proportions[sentiment]\n",
    "        if 0 < n_elem_to_plot <= len(emojis_proportions[sentiment]):\n",
    "            emojis_to_check = dict(sorted(emojis_to_check.items(), key=lambda x: x[1], reverse=True)) # sort by frequency\n",
    "            emojis_to_check = dict(list(emojis_to_check.items())[:n_elem_to_plot]) #take first n_elem_to_plot items from dictionary\n",
    "\n",
    "        wordcloud = WordCloud(width=500, height=500,\n",
    "                              background_color='white',\n",
    "                              min_font_size=10,\n",
    "                              font_path=font_path,\n",
    "                              regexp=regexp).generate_from_frequencies(emojis_to_check)\n",
    "\n",
    "        # plot the WordCloud image\n",
    "        plt.figure(figsize=(5, 5), facecolor=None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.title(\"emojis cloud for sentiment: \" + sentiment)\n",
    "        plt.savefig('./resources/ouput_img/wc_emojis_' + sentiment)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "for sentiment in emoticons_proportions: # Emoticons cloud\n",
    "    if emoticons_proportions.get(sentiment) is not None and len(emoticons_proportions[sentiment]) > 0:\n",
    "        emoticons_to_check = emoticons_proportions[sentiment]\n",
    "        if 0 < n_elem_to_plot <= len(emoticons_proportions[sentiment]):\n",
    "            emoticons_to_check = dict(sorted(emoticons_to_check.items(), key=lambda x: x[1], reverse=True)) # sort by frequency\n",
    "            emoticons_to_check = dict(list(emoticons_to_check.items())[:n_elem_to_plot]) # take first n_elem_to_plot items from dictionary\n",
    "\n",
    "        wordcloud = WordCloud(width=500, height=500,\n",
    "                              background_color='white',\n",
    "                              min_font_size=10,\n",
    "                              font_path=font_path,\n",
    "                              regexp=regexp).generate_from_frequencies(emoticons_to_check)\n",
    "\n",
    "        # plot the WordCloud image\n",
    "        plt.figure(figsize=(5, 5), facecolor=None)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.title(\"emojis cloud for sentiment: \" + sentiment)\n",
    "        plt.savefig('./resources/ouput_img/wc_emoticons_' + sentiment)\n",
    "\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline for goal 2\n",
    "\n",
    "For each lexical results L of sentiment S obtains the percentage of words in L from tweets of sentiment S"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perc_presence_pipeline = [\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'LexResourcesWords',\n",
    "            'localField': '_id',\n",
    "            'foreignField': 'resources.$id',\n",
    "            'as': 'words'\n",
    "        }\n",
    "    }, {\n",
    "        '$unwind': '$words'\n",
    "    }, {\n",
    "        '$group': {\n",
    "            '_id': '$_id',\n",
    "            'lexResWords': {\n",
    "                '$push': '$words.lemma'\n",
    "            },\n",
    "            'totNumberLexWords': {\n",
    "                '$first': '$totNumberWords'\n",
    "            },\n",
    "            'sentiment': {\n",
    "                '$first': '$sentiment'\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$lookup': {\n",
    "            'from': 'Tweets',\n",
    "            'localField': 'sentiment',\n",
    "            'foreignField': 'sentiment',\n",
    "            'as': 'tweets'\n",
    "        }\n",
    "    }, {\n",
    "        '$unwind': '$tweets'\n",
    "    }, {\n",
    "        '$unwind': {\n",
    "            'path': '$tweets.words'\n",
    "        }\n",
    "    }, {\n",
    "        '$group': {\n",
    "            '_id': '$_id',\n",
    "            'tweetsWords': {\n",
    "                '$push': '$tweets.words.lemma'\n",
    "            },\n",
    "            'lexResWords': {\n",
    "                '$first': '$lexResWords'\n",
    "            },\n",
    "            'totNumberLexWords': {\n",
    "                '$first': '$totNumberLexWords'\n",
    "            },\n",
    "            'sentiment': {\n",
    "                '$first': '$sentiment'\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$addFields': {\n",
    "            'numberWordsCommon': {\n",
    "                '$size': {\n",
    "                    '$setIntersection': [\n",
    "                        '$tweetsWords', '$lexResWords'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'totNumberTweetsWords': {\n",
    "                '$size': '$tweetsWords'\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$addFields': {\n",
    "            'percPresenceLexRes': {\n",
    "                '$divide': [\n",
    "                    '$numberWordsCommon', '$totNumberLexWords'\n",
    "                ]\n",
    "            },\n",
    "            'percPresenceTwitter': {\n",
    "                '$divide': [\n",
    "                    '$numberWordsCommon', '$totNumberTweetsWords'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$project': {\n",
    "            '_id': 1,\n",
    "            'sentiment': 1,\n",
    "            'percPresenceLexRes': 1,\n",
    "            'percPresenceTwitter': 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "time_pip2_start = perf_counter()\n",
    "pip2_res = db_lex_res_collection.aggregate(perc_presence_pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_pip2_end = perf_counter()\n",
    "time_pip2 = time_pip2_end - time_pip2_start\n",
    "print(\"Elapsed time for pipeline 2 (proportions): \", str(timedelta(seconds=time_pip2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_pip2_res = list(pip2_res)\n",
    "if ALLOW_PRINT:\n",
    "    pprint(list_pip2_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Histograms\n",
    "\n",
    "For each sentiment S create an istogram in which for each lexical resource L of sentiment S is indicated the proportion of its words contained in the tweets of sentiment S (percPrecenceLexRes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# map to contain for each sentiment S the corresponding list of lexical resources of sentiment S,\n",
    "# each of them associated with the proportion of its words contained in tweets of sentiment S\n",
    "map_sentiment_perc_presence_lex_res = {}\n",
    "for lex_res in list_pip2_res:\n",
    "    # if for that sentiment there still not be an entry for the list of lexical resources+proportion\n",
    "    # then create one else concat the new lexres+proportion\n",
    "    if map_sentiment_perc_presence_lex_res.get(lex_res[\"sentiment\"]) is None:\n",
    "        map_sentiment_perc_presence_lex_res[lex_res[\"sentiment\"]] = [{\"lex_res\": lex_res[\"_id\"], \"perc\": lex_res[\"percPresenceLexRes\"]}]\n",
    "    else:\n",
    "        map_sentiment_perc_presence_lex_res[lex_res[\"sentiment\"]].append({\"lex_res\": lex_res[\"_id\"], \"perc\": lex_res[\"percPresenceLexRes\"]})\n",
    "\n",
    "if ALLOW_PRINT:\n",
    "    print(map_sentiment_perc_presence_lex_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 10), facecolor=None).add_subplot(111)\n",
    "plt.title(\"Proportion of words of lex res present in tweets for sentiments\")\n",
    "#plt.gca().set_ylim([0, 1])\n",
    "plt.tight_layout(pad=0)\n",
    "for sentiment in map_sentiment_perc_presence_lex_res:\n",
    "    x_list = []\n",
    "    proportions = []\n",
    "    for pair_lex_res_perc in map_sentiment_perc_presence_lex_res[sentiment]:\n",
    "        x_list.append(pair_lex_res_perc[\"lex_res\"])\n",
    "        proportions.append(pair_lex_res_perc[\"perc\"])\n",
    "\n",
    "    plt.bar(x_list, proportions, align='center')\n",
    "\n",
    "plt.savefig('./resources/ouput_img/hist')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract words in Tweets but not in Lexical Resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_pip3_start = perf_counter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unknown_words_pipeline = [\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'LexResourcesWords',\n",
    "            'localField': '_id',\n",
    "            'foreignField': 'resources.$id',\n",
    "            'as': 'words'\n",
    "        }\n",
    "    }, {\n",
    "        '$unwind': '$words'\n",
    "    }, {\n",
    "        '$group': {\n",
    "            '_id': '$_id',\n",
    "            'lexResWords': {\n",
    "                '$push': '$words.lemma'\n",
    "            },\n",
    "            'totNumberLexWords': {\n",
    "                '$first': '$totNumberWords'\n",
    "            },\n",
    "            'sentiment': {\n",
    "                '$first': '$sentiment'\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$lookup': {\n",
    "            'from': 'Tweets',\n",
    "            'localField': 'sentiment',\n",
    "            'foreignField': 'sentiment',\n",
    "            'as': 'tweets'\n",
    "        }\n",
    "    }, {\n",
    "        '$unwind': '$tweets'\n",
    "    }, {\n",
    "        '$unwind': {\n",
    "            'path': '$tweets.words'\n",
    "        }\n",
    "    }, {\n",
    "        '$group': {\n",
    "            '_id': '$_id',\n",
    "            'tweetsWords': {\n",
    "                '$push': '$tweets.words.lemma'\n",
    "            },\n",
    "            'lexResWords': {\n",
    "                '$first': '$lexResWords'\n",
    "            },\n",
    "            'totNumberLexWords': {\n",
    "                '$first': '$totNumberLexWords'\n",
    "            },\n",
    "            'sentiment': {\n",
    "                '$first': '$sentiment'\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$unwind': '$lexResWords'\n",
    "    }, {\n",
    "        '$group': {\n",
    "            '_id': '$sentiment',\n",
    "            'tweetsWords': {\n",
    "                '$first': '$tweetsWords'\n",
    "            },\n",
    "            'lexResWords': {\n",
    "                '$push': '$lexResWords'\n",
    "            }\n",
    "        }\n",
    "    }, {\n",
    "        '$addFields': {\n",
    "            'unknownWords': {\n",
    "                '$setDifference': [\n",
    "                    '$tweetsWords', '$lexResWords'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "unknown_words_pipeline_res = db_lex_res_collection.aggregate(unknown_words_pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_pip3_end = perf_counter()\n",
    "time_pip3 = time_pip3_end - time_pip3_start\n",
    "print(\"Elapsed time for pipeline 3 (new lexical resources): \", str(timedelta(seconds=time_pip3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_pip3_res = list(unknown_words_pipeline_res)\n",
    "if ALLOW_PRINT:\n",
    "    pprint(list_pip3_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# map to contain for each sentiment a list of words in tweets but not in lexical resources\n",
    "map_unknown_words_for_sentiment = {}\n",
    "for lex_res in list_pip3_res:\n",
    "    lex_res_sentiment = lex_res[\"_id\"]\n",
    "    # if for that sentiment there still not be an entry for the list of unknown words then create one else concat unknown words coming\n",
    "    if map_unknown_words_for_sentiment.get(lex_res_sentiment) is None:\n",
    "        map_unknown_words_for_sentiment[lex_res_sentiment] = lex_res[\"unknownWords\"]\n",
    "    else:\n",
    "        map_unknown_words_for_sentiment[lex_res_sentiment] = map_unknown_words_for_sentiment[lex_res_sentiment] + lex_res[\"unknownWords\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create lexical resources files containing the new words found"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for sentiment in map_unknown_words_for_sentiment:\n",
    "    filename = \"./resources/lex_res_new/\" + sentiment + \"_new.txt\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    output_file = open(filename, 'w+', encoding='utf-8')\n",
    "    for word in map_unknown_words_for_sentiment[sentiment]:\n",
    "        output_file.write(word + \"\\n\")\n",
    "    output_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Times summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Time summary\")\n",
    "print(\"\\tLoading lexical resources: \", str(timedelta(seconds=time_lex_res)))\n",
    "print(\"\\tLoading tweets: \", str(timedelta(seconds=time_tweets)))\n",
    "print(\"\\tMongo: inserting lexical resources: \", str(timedelta(seconds=time_lex_res_mongo)))\n",
    "print(\"\\tMongo: inserting lexical resources words: \", str(timedelta(seconds=time_lex_res_words_mongo)))\n",
    "print(\"\\tMongo: inserting tweets: \", str(timedelta(seconds=time_tweets_mongo)))\n",
    "print(\"\\tMongo: pipeline 1 (word clouds): \", str(timedelta(seconds=time_pip1)))\n",
    "print(\"\\tMongo: pipeline 2 (proportions lex res in tweets): \", str(timedelta(seconds=time_pip2)))\n",
    "print(\"\\tMongo: pipeline 3 (new lexical resources): \", str(timedelta(seconds=time_pip3)))\n",
    "total_inserting_time_mongo = time_lex_res_mongo+time_lex_res_words_mongo+time_tweets_mongo\n",
    "total_query_time_mongo = time_pip1+time_pip2+time_pip3\n",
    "print(\"\\tMongo: total inserting time: \", str(timedelta(seconds=total_inserting_time_mongo)))\n",
    "print(\"\\tMongo: total query time: \", str(timedelta(seconds=total_query_time_mongo)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SQL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.LexicalResource import LexicalResource\n",
    "from src.MySql import DBConnection, Token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db_connection = DBConnection()\n",
    "# db_connection.connect_to_db()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Insert lexical resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [print(l.word_list, \"\\n\") for l in lex_resources_list]\n",
    "# db_connection.delete_lex_res()\n",
    "# db_connection.insert_lexical_resources(lex_resources_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Insert tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db_connection.delete_tweets()\n",
    "# tweets_list = []\n",
    "# for tweet in tweets_to_info:\n",
    "#     tweets_list.append(tweet)\n",
    "#\n",
    "# [print(tweet) for tweet in tweets_list]\n",
    "# db_connection.insert_tweets(tweets_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# db_connection.delete_tweets()\n",
    "#\n",
    "# foreign_key_query1 = \"SET FOREIGN_KEY_CHECKS = 0;\"\n",
    "# delete_tokens = \"TRUNCATE token;\"\n",
    "# foreign_key_query2 = \"SET FOREIGN_KEY_CHECKS = 1;\"\n",
    "# db_connection.launch_query(foreign_key_query1)\n",
    "# db_connection.launch_query(delete_tokens)\n",
    "# db_connection.launch_query(foreign_key_query2)\n",
    "#\n",
    "# tweet1: Tweet = Tweet(\"USERNAME know what she ain't 🙅👌 don't lol even need to say it he likes it !\", 0, \"Joy\")\n",
    "# tweet2: Tweet = Tweet(\"angry Boella no 😒 Pensa kill lol #armando you ah rip bu it was better 😂\", 0, \"Joy\")\n",
    "# tweet3: Tweet = Tweet(\"angry Pensa is imho imho imho imho imho imho ;( imho imho imho imho imho angry pensa sad #gervaso banana no\", 0, \"Sadness\")\n",
    "#\n",
    "# db_connection.insert_tweets([tweet1, tweet2, tweet3])\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tweet1: Tweet = Tweet(\"USERNAME know what she ain't 🙅👌 don't lol even need to say it he likes it !\", 0, \"Joy\")\n",
    "# tweet2: Tweet = Tweet(\"angry Boella no 😒 😒 Pensa Pensa lol #armando #bellino Pensa kill pensa lol #armando you ah rip 😂 bu it was better 😂\", 0, \"Joy\")\n",
    "# tweet3: Tweet = Tweet(\"angry Pensa is imho imho imho imho imho imho ;( imho imho imho imho imho angry pensa sad #gervaso banana no\", 0, \"Sadness\")\n",
    "#\n",
    "# token_map = tweet2.get_tokens()\n",
    "#\n",
    "# for token in token_map:\n",
    "#     print(token.content, \" - \", token.content_type, \" - \", token_map[token])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}