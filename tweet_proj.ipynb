{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# trovare le parole piÃ¹ frequenti per ogni sentimento\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "# Se ne abbiamo voglia possiamo mettere le emoticons e gli emoji su file e per poi leggerli\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Set\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import pymongo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Punctuation and emojis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "outputs": [],
   "source": [
    "PUNCTUATION_MARKS = [',', '?', '!', '.', ';', ':', '\\\\', '/', '(', ')', '&', ' ', '_', '+', '=', '<', '>', '\"']\n",
    "\n",
    "EMOTICONS_POS = ['B-)', ':)', ':-)', \":')\", \":'-)\", ':D', ':-D', ':\\'-)', \":')\", ':o)', ':]', ':3', ':c)', ':>', '=]',\n",
    "                 '8)', '=)', ':}', ':^)', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', '=-3', '=3', 'B^D',\n",
    "                 ':-))', ':*', ':^*', '( \\'}{\\' )', '^^', '(^_^)', '^-^', \"^.^\", \"^3\\^\", \"\\^L\\^\"]\n",
    "EMOTICONS_NEG = [':(', ':-(', \":'(\", \":'-(\", '>:[', ':-c', ':c', ':-<', ':<', ':-[', ':[', ':{', ':\\'-(', ':\\'(',\n",
    "                 ' _( ', ':\\'[', \"='(\", \"' [\", \"='[\", \":'-<\", \":' <\", \":'<\", \"=' <\", \"='<\", \"T_T\", \"T.T\", \"(T_T)\",\n",
    "                 \"y_y\", \"y.y\", \"(Y_Y)\", \";-;\", \";_;\", \";.;\", \":_:\", \"o .__. o\", \".-.\", \":/\"]\n",
    "EMOTICONS = EMOTICONS_NEG + EMOTICONS_POS\n",
    "\n",
    "EMOJI_POS = [u'\\U0001F601', u'\\U0001F602', u'\\U0001F603', u'\\U0001F604', u'\\U0001F605', u'\\U0001F606', u'\\U0001F609',\n",
    "             u'\\U0001F60A', u'\\U0001F60B', u'\\U0001F60E', u'\\U0001F60D', u'\\U0001F618', u'\\U0001F617', u'\\U0001F619',\n",
    "             u'\\U0001F61A', u'\\U0000263A', u'\\U0001F642', u'\\U0001F917', u'\\U0001F607', u'\\U0001F60F', u'\\U0001F61C',\n",
    "             u'\\U0001F608', u'\\U0001F646', u'\\U0001F48F', u'\\U0001F44C', u'\\U0001F44F', u'\\U0001F48B', u'\\U0001F638',\n",
    "             u'\\U0001F639', u'\\U0001F63A', u'\\U0001F63B', u'\\U0001F63C', u'\\U0001F63D', u'\\U0001F192', u'\\U0001F197']\n",
    "EMOJI_NEG = [u'\\U0001F625', u'\\U0001F60C', u'\\U00002639', u'\\U0001F641', u'\\U0001F612', u'\\U0001F614', u'\\U0001F615',\n",
    "             u'\\U0001F616', u'\\U0001F632', u'\\U0001F61E', u'\\U0001F61F', u'\\U0001F622', u'\\U0001F62D', u'\\U0001F626',\n",
    "             u'\\U0001F627', u'\\U0001F628', u'\\U0001F631', u'\\U0001F621', u'\\U0001F620', u'\\U0001F64D', u'\\U0001F64E',\n",
    "             u'\\U0000270A', u'\\U0001F44A', u'\\U0001F494', u'\\U0001F4A2', u'\\U0001F5EF', u'\\U0001F63E', u'\\U0001F63F']\n",
    "OTHER_EMOJIS = [u'\\U0001F004', u'\\U0001F0CF', u'\\U0001F300', u'\\U0001F301', u'\\U0001F302', u'\\U0001F303', u'\\U0001F304',\n",
    "                u'\\U0001F305', u'\\U0001F306', u'\\U0001F307', u'\\U0001F309', u'\\U0001F30A', u'\\U0001F30B', u'\\U0001F30F',\n",
    "                u'\\U0001F313', u'\\U0001F315', u'\\U0001F31B', u'\\U0001F320', u'\\U0001F330', u'\\U0001F331', u'\\U0001F334',\n",
    "                u'\\U0001F337', u'\\U0001F338', u'\\U0001F339', u'\\U0001F33A', u'\\U0001F33B', u'\\U0001F33C', u'\\U0001F33D',\n",
    "                u'\\U0001F33E', u'\\U0001F33F', u'\\U0001F340', u'\\U0001F341', u'\\U0001F342', u'\\U0001F343', u'\\U0001F344',\n",
    "                u'\\U0001F345', u'\\U0001F346', u'\\U0001F347', u'\\U0001F348', u'\\U0001F349', u'\\U0001F34C', u'\\U0001F34D',\n",
    "                u'\\U0001F34E', u'\\U0001F34F', u'\\U0001F351', u'\\U0001F352', u'\\U0001F353', u'\\U0001F355', u'\\U0001F356',\n",
    "                u'\\U0001F357', u'\\U0001F358', u'\\U0001F35A', u'\\U0001F35B', u'\\U0001F35C', u'\\U0001F35D', u'\\U0001F35E',\n",
    "                u'\\U0001F35F', u'\\U0001F360', u'\\U0001F361', u'\\U0001F362', u'\\U0001F363', u'\\U0001F364', u'\\U0001F366',\n",
    "                u'\\U0001F367', u'\\U0001F368', u'\\U0001F369', u'\\U0001F36A', u'\\U0001F36B', u'\\U0001F36C', u'\\U0001F36D',\n",
    "                u'\\U0001F36E', u'\\U0001F36F', u'\\U0001F371', u'\\U0001F372', u'\\U0001F373', u'\\U0001F374', u'\\U0001F375',\n",
    "                u'\\U0001F376', u'\\U0001F377', u'\\U0001F378', u'\\U0001F37A', u'\\U0001F37B', u'\\U0001F380', u'\\U0001F381',\n",
    "                u'\\U0001F382', u'\\U0001F384', u'\\U0001F385', u'\\U0001F386', u'\\U0001F387', u'\\U0001F388', u'\\U0001F389',\n",
    "                u'\\U0001F38A', u'\\U0001F38B', u'\\U0001F38C', u'\\U0001F38D', u'\\U0001F38E', u'\\U0001F38F', u'\\U0001F390',\n",
    "                u'\\U0001F391', u'\\U0001F392', u'\\U0001F393', u'\\U0001F3A0', u'\\U0001F3A1', u'\\U0001F3A2', u'\\U0001F3A3',\n",
    "                u'\\U0001F3A4', u'\\U0001F3A5', u'\\U0001F3A6', u'\\U0001F3A7', u'\\U0001F3A8', u'\\U0001F3A9', u'\\U0001F3AA',\n",
    "                u'\\U0001F3AB', u'\\U0001F3AC', u'\\U0001F3AD', u'\\U0001F3AE', u'\\U0001F3AF', u'\\U0001F3B0', u'\\U0001F3B1',\n",
    "                u'\\U0001F3B2', u'\\U0001F3B3', u'\\U0001F3B4', u'\\U0001F3B5', u'\\U0001F3B6', u'\\U0001F3B7', u'\\U0001F3B8',\n",
    "                u'\\U0001F3B9', u'\\U0001F3BA', u'\\U0001F3BB', u'\\U0001F3BC', u'\\U0001F3BD', u'\\U0001F3BE', u'\\U0001F3BF',\n",
    "                u'\\U0001F3C0', u'\\U0001F3C1', u'\\U0001F3C2', u'\\U0001F3C3', u'\\U0001F3C4', u'\\U0001F3C6', u'\\U0001F3C8',\n",
    "                u'\\U0001F3CA', u'\\U0001F3E0', u'\\U0001F3E1', u'\\U0001F3E2', u'\\U0001F3E3', u'\\U0001F3E5', u'\\U0001F3E6',\n",
    "                u'\\U0001F3E7', u'\\U0001F3E8', u'\\U0001F3E9', u'\\U0001F3EA', u'\\U0001F3EB', u'\\U0001F3EC', u'\\U0001F3ED',\n",
    "                u'\\U0001F3EE', u'\\U0001F3EF', u'\\U0001F3F0', u'\\U0001F40C', u'\\U0001F40D', u'\\U0001F40E', u'\\U0001F411',\n",
    "                u'\\U0001F412', u'\\U0001F414', u'\\U0001F417', u'\\U0001F418', u'\\U0001F419', u'\\U0001F41A', u'\\U0001F41B',\n",
    "                u'\\U0001F41C', u'\\U0001F41D', u'\\U0001F41E', u'\\U0001F41F', u'\\U0001F420', u'\\U0001F421', u'\\U0001F422',\n",
    "                u'\\U0001F423', u'\\U0001F424', u'\\U0001F425', u'\\U0001F426', u'\\U0001F427', u'\\U0001F428', u'\\U0001F429',\n",
    "                u'\\U0001F42B', u'\\U0001F42C', u'\\U0001F42D', u'\\U0001F42E', u'\\U0001F42F', u'\\U0001F430', u'\\U0001F431',\n",
    "                u'\\U0001F432', u'\\U0001F433', u'\\U0001F434', u'\\U0001F435', u'\\U0001F436', u'\\U0001F437', u'\\U0001F438',\n",
    "                u'\\U0001F439', u'\\U0001F43A', u'\\U0001F43B', u'\\U0001F43C', u'\\U0001F43D', u'\\U0001F43E', u'\\U0001F440',\n",
    "                u'\\U0001F442', u'\\U0001F443', u'\\U0001F444', u'\\U0001F445', u'\\U0001F446', u'\\U0001F447', u'\\U0001F448',\n",
    "                u'\\U0001F449', u'\\U0001F44A', u'\\U0001F44B', u'\\U0001F44C', u'\\U0001F44D', u'\\U0001F44E', u'\\U0001F44F',\n",
    "                u'\\U0001F450', u'\\U0001F451', u'\\U0001F452', u'\\U0001F453', u'\\U0001F454', u'\\U0001F455', u'\\U0001F456',\n",
    "                u'\\U0001F457', u'\\U0001F458', u'\\U0001F459', u'\\U0001F45A', u'\\U0001F45B', u'\\U0001F45C', u'\\U0001F45D',\n",
    "                u'\\U0001F45E', u'\\U0001F45F', u'\\U0001F460', u'\\U0001F461', u'\\U0001F462', u'\\U0001F463', u'\\U0001F464',\n",
    "                u'\\U0001F466', u'\\U0001F467', u'\\U0001F468', u'\\U0001F469', u'\\U0001F46A', u'\\U0001F46B', u'\\U0001F46E',\n",
    "                u'\\U0001F46F', u'\\U0001F470', u'\\U0001F471', u'\\U0001F472', u'\\U0001F473', u'\\U0001F474', u'\\U0001F475',\n",
    "                u'\\U0001F476', u'\\U0001F477', u'\\U0001F478', u'\\U0001F479', u'\\U0001F47A', u'\\U0001F47B', u'\\U0001F47C',\n",
    "                u'\\U0001F47D', u'\\U0001F47E', u'\\U0001F47F', u'\\U0001F480', u'\\U0001F481', u'\\U0001F482', u'\\U0001F483',\n",
    "                u'\\U0001F484', u'\\U0001F485', u'\\U0001F486', u'\\U0001F487', u'\\U0001F488', u'\\U0001F489', u'\\U0001F48A',\n",
    "                u'\\U0001F48B', u'\\U0001F48C', u'\\U0001F48D', u'\\U0001F48E', u'\\U0001F48F', u'\\U0001F490', u'\\U0001F491',\n",
    "                u'\\U0001F492', u'\\U0001F493', u'\\U0001F494', u'\\U0001F495', u'\\U0001F496', u'\\U0001F497', u'\\U0001F498',\n",
    "                u'\\U0001F499', u'\\U0001F49A', u'\\U0001F49B', u'\\U0001F49C', u'\\U0001F49D', u'\\U0001F49E', u'\\U0001F49F',\n",
    "                u'\\U0001F4A0', u'\\U0001F4A1', u'\\U0001F4A2', u'\\U0001F4A3', u'\\U0001F4A4', u'\\U0001F4A5', u'\\U0001F4A6',\n",
    "                u'\\U0001F4A7', u'\\U0001F4A8', u'\\U0001F4A9', u'\\U0001F4AA', u'\\U0001F4AB', u'\\U0001F4AC', u'\\U0001F4AE',\n",
    "                u'\\U0001F4AF', u'\\U0001F4B0', u'\\U0001F4B1', u'\\U0001F4B2', u'\\U0001F4B3', u'\\U0001F4B4', u'\\U0001F4B5',\n",
    "                u'\\U0001F4B8', u'\\U0001F4B9', u'\\U0001F4BA', u'\\U0001F4BB', u'\\U0001F4BC', u'\\U0001F4BD', u'\\U0001F4BE',\n",
    "                u'\\U0001F4BF', u'\\U0001F4C0', u'\\U0001F4C1', u'\\U0001F4C2', u'\\U0001F4C3', u'\\U0001F4C4', u'\\U0001F4C5',\n",
    "                u'\\U0001F4C6', u'\\U0001F4C7', u'\\U0001F4C8', u'\\U0001F4C9', u'\\U0001F4CA', u'\\U0001F4CB', u'\\U0001F4CC',\n",
    "                u'\\U0001F4CD', u'\\U0001F4CE', u'\\U0001F4CF', u'\\U0001F4D0', u'\\U0001F4D1', u'\\U0001F4D2', u'\\U0001F4D3',\n",
    "                u'\\U0001F4D4', u'\\U0001F4D5', u'\\U0001F4D6', u'\\U0001F4D7', u'\\U0001F4D8', u'\\U0001F4D9', u'\\U0001F4DA',\n",
    "                u'\\U0001F4DB', u'\\U0001F4DC', u'\\U0001F4DD', u'\\U0001F4DE', u'\\U0001F4DF', u'\\U0001F4E0', u'\\U0001F4E1',\n",
    "                u'\\U0001F4E2', u'\\U0001F4E3', u'\\U0001F4E4', u'\\U0001F4E5', u'\\U0001F4E6', u'\\U0001F4E7', u'\\U0001F4E8',\n",
    "                u'\\U0001F4E9', u'\\U0001F4EA', u'\\U0001F4EB', u'\\U0001F4EE', u'\\U0001F4F0', u'\\U0001F4F1', u'\\U0001F4F2',\n",
    "                u'\\U0001F4F3', u'\\U0001F4F4', u'\\U0001F4F6', u'\\U0001F4F7', u'\\U0001F4F9', u'\\U0001F4FA', u'\\U0001F4FB',\n",
    "                u'\\U0001F4FC', u'\\U0001F503', u'\\U0001F50A', u'\\U0001F50B', u'\\U0001F50C', u'\\U0001F50D', u'\\U0001F50E',\n",
    "                u'\\U0001F50F', u'\\U0001F510', u'\\U0001F511', u'\\U0001F512', u'\\U0001F513', u'\\U0001F514', u'\\U0001F516',\n",
    "                u'\\U0001F517', u'\\U0001F518', u'\\U0001F519', u'\\U0001F51A', u'\\U0001F51B', u'\\U0001F51C', u'\\U0001F51D',\n",
    "                u'\\U0001F51E', u'\\U0001F51F', u'\\U0001F520', u'\\U0001F521', u'\\U0001F522', u'\\U0001F523', u'\\U0001F524',\n",
    "                u'\\U0001F525', u'\\U0001F526', u'\\U0001F527', u'\\U0001F528', u'\\U0001F529', u'\\U0001F52A', u'\\U0001F52B',\n",
    "                u'\\U0001F52E', u'\\U0001F52F', u'\\U0001F530', u'\\U0001F531', u'\\U0001F532', u'\\U0001F533', u'\\U0001F534',\n",
    "                u'\\U0001F535', u'\\U0001F536', u'\\U0001F537', u'\\U0001F538', u'\\U0001F539', u'\\U0001F53A', u'\\U0001F53B',\n",
    "                u'\\U0001F53C', u'\\U0001F53D', u'\\U0001F550', u'\\U0001F551', u'\\U0001F552', u'\\U0001F553', u'\\U0001F554',\n",
    "                u'\\U0001F555', u'\\U0001F556', u'\\U0001F557', u'\\U0001F558', u'\\U0001F559', u'\\U0001F55A', u'\\U0001F55B',\n",
    "                u'\\U0001F5FB', u'\\U0001F5FC', u'\\U0001F5FD', u'\\U0001F5FE', u'\\U0001F5FF', u'\\U0001F601', u'\\U0001F602',\n",
    "                u'\\U0001F603', u'\\U0001F604', u'\\U0001F605', u'\\U0001F606', u'\\U0001F609', u'\\U0001F60F', u'\\U0001F612',\n",
    "                u'\\U0001F613', u'\\U0001F61C', u'\\U0001F61D', u'\\U0001F61E', u'\\U0001F620', u'\\U0001F621', u'\\U0001F622',\n",
    "                u'\\U0001F623', u'\\U0001F624', u'\\U0001F625', u'\\U0001F628', u'\\U0001F629', u'\\U0001F62A', u'\\U0001F62B',\n",
    "                u'\\U0001F630', u'\\U0001F631', u'\\U0001F632', u'\\U0001F633', u'\\U0001F635', u'\\U0001F637', u'\\U0001F638',\n",
    "                u'\\U0001F639', u'\\U0001F63A', u'\\U0001F63B', u'\\U0001F63C', u'\\U0001F63D', u'\\U0001F63E', u'\\U0001F63F',\n",
    "                u'\\U0001F640', u'\\U0001F645', u'\\U0001F646', u'\\U0001F647', u'\\U0001F648', u'\\U0001F649', u'\\U0001F64A',\n",
    "                u'\\U0001F64B', u'\\U0001F64C', u'\\U0001F64E', u'\\U0001F64F', u'\\U0001F64F']\n",
    "# AdditionalEmoji=[u'\\U+203C',u'\\U+2049', u'\\U+231A',u'\\U+231B',u'\\U+2600',u'\\U+2601',u'\\U+260E',u'\\U+2611',u'\\U+2614',u'\\U+2615',u'\\U+261D',u'\\U+2648',u'\\U+2648',u'\\U+2649',u'\\U+264A',u'\\U+264B',u'\\U+264C',u'\\U+264D',u'\\U+264E',u'\\U+264F',u'\\U+2650',u'\\U+2651',u'\\U+2652',u'\\U+2653',u'\\U+2660',u'\\U+2663',u'\\U+2665',u'\\U+2666',u'\\U+2668',u'\\U+267B',u'\\U+267F',u'\\U+2693',u'\\U+26A0',u'\\U+26A1',u'\\U+26AA',u'\\U+26AB',u'\\U+26BD',u'\\U+26BE',u'\\U+26C4',u'\\U+26C5',u'\\U+26CE',u'\\U+26D4',u'\\U+26EA',u'\\U+26F2',u'\\U+26F3',u'\\U+26F5',u'\\U+26FA',u'\\U+26FD',u'\\U+2934',u'\\U+2935',u'\\U+2934',u'\\U+2B05',u'\\U+2B06',u'\\U+2B07',u'\\U+2B50',u'\\U+2B55',u'\\U+2B50']\n",
    "EMOJIS = EMOJI_POS + EMOJI_NEG + OTHER_EMOJIS\n",
    "\n",
    "SLANGS = {'afaik': 'as far as i know', 'afk': 'away from keyboard', 'asap': 'as soon as possible',\n",
    "          'atk': 'at the keyboard', 'atm': 'at the moment', 'a3': 'anytime, anywhere, anyplace',\n",
    "          'bak': 'back at keyboard', 'bbl': 'be back later', 'bbs': 'be back soon', 'bfn/b4n': 'bye for now',\n",
    "          'brb': 'be right back', 'brt': 'be right there', 'btw': 'by the way', 'b4n': 'bye for now', 'cu': 'see you',\n",
    "          'cul8r': 'see you later', 'cya': 'see you', 'faq': 'frequently asked questions', 'fc': 'fingers crossed',\n",
    "          'fwiw': 'for what it\\'s worth', 'fyi': 'for your information', 'gal': 'get a life', 'gg': 'good game',\n",
    "          'gmta': 'great minds think alike', 'gr8': 'great!', 'g9': 'genius', 'ic': 'i see', 'icq': 'i seek you',\n",
    "          'ilu': 'ilu: i love you', 'imho': 'in my honest opinion', 'imo': 'in my opinion', 'iow': 'in other words',\n",
    "          'irl': 'in real life', 'kiss': 'keep it simple, stupid', 'ldr': 'long distance relationship',\n",
    "          'lmao': 'laugh my a.. off', 'lol': 'laughing out loud', 'ltns': 'long time no see', 'l8r': 'later',\n",
    "          'mte': 'my thoughts exactly', 'm8': 'mate', 'nrn': 'no reply necessary', 'oic': 'oh i see',\n",
    "          'pita': 'pain in the a..', 'prt': 'party', 'prw': 'parents are watching', 'qpsa?': 'que pasa?',\n",
    "          'rofl': 'rolling on the floor laughing', 'roflol': 'rolling on the floor laughing out loud',\n",
    "          'rotflmao': 'rolling on the floor laughing my a.. off', 'sk8': 'skate', 'stats': 'your sex and age',\n",
    "          'asl': 'age, sex, location', 'thx': 'thank you', 'ttfn': 'ta-ta for now!', 'ttyl': 'talk to you later',\n",
    "          ' u': ' you', 'u ': 'you ', 'u.': 'you.', 'u2': 'you too', 'u4e': 'yours for ever', 'wb': 'welcome back', 'wtf': 'what the f...',\n",
    "          'wtg': 'way to go!', 'wuf': 'where are you from?', 'w8': 'wait...', '7k': 'sick:-d laugher'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Class used to store information about each tweet: its sentiment e how many words of which sentiment are in that tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "outputs": [],
   "source": [
    "class TweetInfo:\n",
    "    sentiment: str\n",
    "    sentiment_occurrences: Dict[str, int]\n",
    "\n",
    "    def __init__(self, sentiment: str, sentiments_for_words: List[str]):\n",
    "        self.sentiment = sentiment\n",
    "\n",
    "        self.sentiment_occurrences = {}\n",
    "        for sentiment_for_words in sentiments_for_words:\n",
    "            # initiate every word sentiment occurrence to 0\n",
    "            self.sentiment_occurrences[sentiment_for_words] = 0\n",
    "\n",
    "    def increase_sentiment_counter(self, sentiment: str):\n",
    "        self.sentiment_occurrences[sentiment] = self.sentiment_occurrences.get(sentiment) + 1\n",
    "\n",
    "    def print_tweet_info(self):\n",
    "        print(\"tweet sentiment: \", self.sentiment)\n",
    "        for word_sentiment in self.sentiment_occurrences:\n",
    "            print(\"\\t word sentiment: \", word_sentiment)\n",
    "            print(\"\\t occurrences: \", self.sentiment_occurrences[word_sentiment])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "outputs": [],
   "source": [
    "class LexicalResource:\n",
    "    filename: str\n",
    "    sentiment: str\n",
    "    word_list: List[str]\n",
    "\n",
    "    def __init__(self, filename: str, sentiment: str):\n",
    "        self.filename = filename\n",
    "        self.sentiment = sentiment\n",
    "        self.word_list = []\n",
    "\n",
    "    def __str__(self):\n",
    "        lex_res_string = \"LexicalResource: \" + self.filename + \\\n",
    "                         \"\\n\\t sentiment: \" + self.sentiment + \\\n",
    "                         \"\\n\\t wordlist: \" + self.word_list.__str__()\n",
    "        return lex_res_string\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        if not '_' in word:\n",
    "            self.word_list.append(word)\n",
    "\n",
    "    def add_word_list(self, word_list: List[str]):\n",
    "        for word in word_list:\n",
    "            if not '_' in word:\n",
    "                self.word_list.append(word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    text: str\n",
    "    hashtags: List[str]\n",
    "    emojis: List[str]\n",
    "    emoticons: List[str]\n",
    "    tokens: List[str]\n",
    "    words: List[str]\n",
    "    pos_tags: Dict[str, str]\n",
    "    tweet_stem_count: TweetInfo\n",
    "\n",
    "    def __init__(self, tweet_raw: str):\n",
    "        self.text = tweet_raw\n",
    "        self.anonymize()\n",
    "        self.read_hashtags()\n",
    "        self.read_emojis()\n",
    "        self.read_emoticons()\n",
    "        self.to_lower()\n",
    "        self.tokenize()\n",
    "        self.process_slangs()\n",
    "        self.pos_tagging()\n",
    "        self.remove_punctuation()\n",
    "        self.lemming()\n",
    "        self.remove_stop_words()\n",
    "\n",
    "    # TODO fai il to string al posto del print tweet\n",
    "    def __str__(self):\n",
    "        tweet_string = \"Tweet\\n\"\n",
    "        tweet_string = tweet_string + \"\\ttweet raw: \" + self.text\n",
    "        tweet_string = tweet_string + \"\\tpos tags: \" + json.dumps(self.pos_tags)\n",
    "        tweet_string = tweet_string + \"\\n\"\n",
    "        return tweet_string\n",
    "\n",
    "    def read_hashtags(self) -> None:\n",
    "        self.hashtags = re.findall(r\"#(\\w+)\", self.text)\n",
    "\n",
    "    def read_emojis(self) -> None:\n",
    "        self.emojis = get_elems_from_text_if_in_list(self.text, EMOJIS)\n",
    "\n",
    "    def read_emoticons(self) -> None:\n",
    "        self.emoticons = get_elems_from_text_if_in_list(self.text, EMOTICONS)\n",
    "\n",
    "    def anonymize(self) -> None:\n",
    "        self.text = self.text.replace(\"USERNAME\", \"\").replace(\"URL\", \"\")\n",
    "\n",
    "    def to_lower(self) -> None:\n",
    "        self.text = self.text.lower()\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        # Questa funzione mi sa che non andava bene, poi vediamo\n",
    "        self.tokens = nltk.word_tokenize(self.text)\n",
    "        # self.tokens = sent_tokenize(self.text)\n",
    "\n",
    "    def pos_tagging(self) -> None:\n",
    "        pos_tag_list = nltk.pos_tag(self.tokens)\n",
    "\n",
    "        # get keys of pos tag list\n",
    "        pos_tag_keys = [pos_tag[0] for pos_tag in pos_tag_list]\n",
    "        # get values of pos tag list\n",
    "        pos_tag_values = [pos_tag[1] for pos_tag in pos_tag_list]\n",
    "        # create dictionary\n",
    "        pos_tag_dict: Dict[str, str] = {pos_tag_keys[i]: pos_tag_values[i] for i in range(len(pos_tag_keys))}\n",
    "\n",
    "        self.pos_tags = pos_tag_dict\n",
    "\n",
    "    def remove_punctuation(self) -> None:\n",
    "        # Removes every character besides lower and uppercase letters, numbers and spaces\n",
    "        # self.text = re.sub(r'[^a-zA-Z0-9 ]', '', self.text)\n",
    "        for tag_key in list(self.pos_tags.keys()):\n",
    "            if tag_key in PUNCTUATION_MARKS:\n",
    "                # tag key is a punctuation mark, so remove from pos tagging list\n",
    "                del self.pos_tags[tag_key]\n",
    "\n",
    "    def print_tweet(self) -> None:\n",
    "        print(\"tweet raw: \", self.text)\n",
    "        print(\"pos tagging: \", self.pos_tags)\n",
    "        # print(\"\\n\\ttokens \", self.tokens)\n",
    "        # print(\"\\n\\thashtag_list \", self.hashtags)\n",
    "        # print(\"\\n\\temoji_list \", self.emojis)\n",
    "        # print(\"\\n\\temoticon_list \", self.emoticons)\n",
    "        # print(\"\\n\\twords_list \", self.get_words())\n",
    "\n",
    "    def lemming(self) -> None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tweet_words_lemmatized: List[str] = []\n",
    "\n",
    "        tweet_words = self.get_words()\n",
    "        for word in tweet_words:\n",
    "            tweet_words_lemmatized.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    def remove_stop_words(self) -> None:\n",
    "\n",
    "        # TODO implement this\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        tokens = self.tokens\n",
    "\n",
    "        for w in tokens:\n",
    "            if w in stop_words:\n",
    "                tokens.remove(w)\n",
    "\n",
    "        self.words = tokens\n",
    "\n",
    "    def process_slangs(self) -> None:\n",
    "        for slang in SLANGS:\n",
    "            # replaces the slang with the extension for every slang in the text\n",
    "            self.text = self.text.replace(slang, SLANGS[slang])\n",
    "\n",
    "    # Support functions\n",
    "\n",
    "    def get_words(self) -> List[str]:\n",
    "        return self.text.split()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "outputs": [],
   "source": [
    "def get_elems_from_text_if_in_list(text: str, list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return list of substrings of text that appear in list\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in list:\n",
    "            matches.append(word)\n",
    "\n",
    "    return matches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "outputs": [],
   "source": [
    "lex_resources_directory = \"resources/test/lex_res\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Read files in directory\n",
    "General function to read text files from a directory and merge them"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [],
   "source": [
    "lex_resources_list: List[LexicalResource] = []\n",
    "\n",
    "def read_texts_in_directory(directory_path: str, sentiment: str) -> List[str]:\n",
    "    files_text_list: List[str] = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            file = open(file=file_path, encoding=\"utf8\")\n",
    "            file_text = file.read().split() # list of words for a single lex resource of a sentiment\n",
    "            files_text_list = files_text_list + file_text\n",
    "\n",
    "            lex_res: LexicalResource = LexicalResource(filename, sentiment)\n",
    "            lex_res.add_word_list(file_text)\n",
    "            global lex_resources_list\n",
    "            lex_resources_list.append(lex_res)\n",
    "\n",
    "    # print(len(lex_resources_list))\n",
    "    # [print(i) for i in lex_resources_list]\n",
    "    return files_text_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Read lexical resources for a sentiment\n",
    "Function which reads all the lexical resources for a sentiment\n",
    "The directory containing all lexical resources for that sentiment is passed as parameter\n",
    "\n",
    "### forse creare per ogni lex res di OGNI sentimento un dizionario diverso? Bisogna vedere come caricare i dati su db, bisogna caricare ogni lex res diversa di ogni sentimento sul db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [],
   "source": [
    "def read_lex_resources_sentiment(sentiment_lex_resources_directory: str, sentiment: str) -> Set[str]:\n",
    "    resource_words: Set[str] = set()\n",
    "    resources_text: List[str] = read_texts_in_directory(sentiment_lex_resources_directory, sentiment)\n",
    "    for word in resources_text:\n",
    "        if not '_' in word:\n",
    "            resource_words.add(word)\n",
    "    #print(sentiment, \"\\n\", resource_words, \"\\n\\n\")\n",
    "    return resource_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read all lexical resources\n",
    "\n",
    "Reads all the lexical resources and returns a dictionary of word to sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [],
   "source": [
    "sentiment_lex_resources: Dict[str, str] = {}\n",
    "\n",
    "for resources_path, sentiments, _ in os.walk(lex_resources_directory):\n",
    "    # The folders inside the lexical resources folder are named after a sentiment (Ex. Anger, Joy), each of them contain some files and each of them is a list of words that are associated with that sentiment\n",
    "    \"\"\"WARN Ad ogni ciclo ci sarebbe per forza un singolo sentiment dato che cicla sulla directory delle directory di lex res?\"\"\"\n",
    "    for sentiment in sentiments:\n",
    "        # iterate each folder (one for sentiment)\n",
    "        resources_sentiment_path = os.path.join(resources_path, sentiment)\n",
    "        sentiment_words_set: Set[str] = read_lex_resources_sentiment(resources_sentiment_path, sentiment)\n",
    "\n",
    "        # read the files containing lists of words, and return a set of all the words in those files\n",
    "        for sentiment_word in sentiment_words_set:\n",
    "            # associate each word of the set to the corresponding sentiment\n",
    "            sentiment_lex_resources[sentiment_word] = sentiment\n",
    "\n",
    "lex_word_to_sentiment = sentiment_lex_resources\n",
    "# print(len(lex_resources_list))\n",
    "# [print(i) for i in lex_resources_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tweet reading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "outputs": [],
   "source": [
    "tweets_directory = \"resources/test/tweets/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Reads a file and converts the text to tweets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "outputs": [],
   "source": [
    "def read_tweet_file(file_path_string: str) -> List[Tweet]:\n",
    "    \"\"\"\n",
    "    Reads a file and converts the text to tweets\n",
    "    :param file_path_string: string of the path to the file\n",
    "    \"\"\"\n",
    "\n",
    "    # tweets read from file\n",
    "    tweets_read: List[Tweet] = []\n",
    "\n",
    "    tweets_file = open(file=file_path_string, encoding=\"utf8\")\n",
    "    tweets_text: List[str] = tweets_file.readlines()\n",
    "\n",
    "    # For each tweet text create a Tweet object\n",
    "    for tweet_text in tweets_text:\n",
    "        new_tweet = Tweet(tweet_text)\n",
    "        tweets_read.append(new_tweet)\n",
    "\n",
    "    return tweets_read"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [],
   "source": [
    "def get_tweet_sentiment_from_file_name(file_name: str):\n",
    "    extension_removed = file_name.split(\".\")[0]\n",
    "    sentiment = extension_removed.split(\"_\")[-2]\n",
    "    return sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get list of sentiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [],
   "source": [
    "sentiments: List[str] = [sentiment for sentiment in os.listdir(lex_resources_directory)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read tweets folder and load Tweet Info for stem counting\n",
    "The tweets folder contains for each sentiment a file containing tweets of that sentiment. Each file is scanned and for each tweet a TweetInfo object is created in order to maintain the count of how many word of which sentiments are in it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ma associare il TweetInfo al tweet senza fare un altro dict?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [],
   "source": [
    "tweets_to_info: Dict[Tweet, TweetInfo] = {}\n",
    "for tweets_sentiments_directory, _, tweets_sentiments_filenames in os.walk(tweets_directory):\n",
    "    # print(tweets_sentiments_directory)\n",
    "    # print(tweets_sentiments_files)\n",
    "\n",
    "    for tweets_sentiment_filename in tweets_sentiments_filenames:\n",
    "        # print(tweets_sentiment_file)\n",
    "        tweets_sentiment_filepath = os.path.join(tweets_sentiments_directory, tweets_sentiment_filename)\n",
    "        sentiment = get_tweet_sentiment_from_file_name(tweets_sentiment_filename)\n",
    "        tweets_for_sentiment: List[Tweet] = read_tweet_file(tweets_sentiment_filepath)\n",
    "        #print(\"Tweets for sentiment: \", sentiment, \"\\n\")\n",
    "        for tweet in tweets_for_sentiment:\n",
    "            #print(tweet)\n",
    "            tweet_info: TweetInfo = TweetInfo(sentiment, sentiments)\n",
    "            tweets_to_info[tweet] = tweet_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stem counting\n",
    "For each tweet and each word of them is checked the sentiment and increased the counter for that sentiment in the TweetInfo object associated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "outputs": [],
   "source": [
    "for tweet in tweets_to_info:\n",
    "    tweet_info = tweets_to_info[tweet]\n",
    "    tweet_words: List[str] = tweet.get_words()\n",
    "\n",
    "    for word in tweet_words:\n",
    "        if word in lex_word_to_sentiment:\n",
    "            # get the sentiment for the word and increase sentiment counter by 1\n",
    "            sentiment = lex_word_to_sentiment[word]\n",
    "            tweet_info.increase_sentiment_counter(sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "outputs": [],
   "source": [
    "def print_tweets():\n",
    "    for tweet in tweets_to_info.keys():\n",
    "        info = tweets_to_info[tweet]\n",
    "        print(tweet)\n",
    "        print(\"sentiment: \" + info.sentiment)\n",
    "        print(\"sentiment occurrences: \")\n",
    "        print(info.sentiment_occurrences)\n",
    "        print(\"---\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet\n",
      "\ttweet raw:  yes the cillo is very chill smoking on the balcony ;)\n",
      "\tpos tags: {\"yes\": \"RB\", \"the\": \"DT\", \"cillo\": \"NN\", \"is\": \"VBZ\", \"very\": \"RB\", \"chill\": \"JJ\", \"smoking\": \"VBG\", \"on\": \"IN\", \"balcony\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: wow i'm having fun doing the smoking\n",
      "\tpos tags: {\"wow\": \"NN\", \"i\": \"NN\", \"'m\": \"VBP\", \"having\": \"VBG\", \"fun\": \"NN\", \"doing\": \"VBG\", \"the\": \"DT\", \"smoking\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: yea  yea\n",
      "\tpos tags: {\"yea\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: boss\tpos tags: {\"boss\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: angry pensa is angry sad banana no\n",
      "\tpos tags: {\"angry\": \"JJ\", \"pensa\": \"NN\", \"is\": \"VBZ\", \"sad\": \"JJ\", \"banana\": \"NN\", \"no\": \"DT\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 3, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: angry boella no pensa kill ;( yoyou ah rip bu\n",
      "\tpos tags: {\"angry\": \"JJ\", \"boella\": \"NN\", \"no\": \"DT\", \"pensa\": \"NN\", \"kill\": \"NN\", \"you\": \"PRP\", \"ah\": \"VBP\", \"rip\": \"JJ\", \"bu\": \"NN\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 3, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw:  know what she ain't ðŸ˜’ don't even need to say it !\tpos tags: {\"know\": \"VB\", \"what\": \"WP\", \"she\": \"PRP\", \"ai\": \"VBP\", \"n't\": \"RB\", \"\\ud83d\\ude12\": \"NNP\", \"do\": \"VBP\", \"even\": \"RB\", \"need\": \"VB\", \"to\": \"TO\", \"say\": \"VB\", \"it\": \"PRP\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"Tweets with tweet info\\n\")\n",
    "#\n",
    "# for tweet in tweets_to_info:\n",
    "#     tweet.print_tweet()\n",
    "#     tweet_info = tweets_to_info[tweet]\n",
    "#     tweet_info.print_tweet_info()\n",
    "#     print(\"\\n\\n\")\n",
    "\n",
    "print_tweets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MongoDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexicalResource: angry\n",
      "\t sentiment: Anger\n",
      "\t wordlist: ['no', 'Pensa', 'angry']\n",
      "LexicalResource: pensa_angry.txt\n",
      "\t sentiment: Anger\n",
      "\t wordlist: ['bbu', 'bubu', 'ya', 'bu']\n",
      "LexicalResource: happy\n",
      "\t sentiment: Joy\n",
      "\t wordlist: ['Yes', 'Boella', 'privacy', 'Pensa']\n"
     ]
    },
    {
     "data": {
      "text/plain": "[None, None, None]"
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_connected = True\n",
    "if not already_connected:\n",
    "    mongo_client = pymongo.MongoClient(\"mongodb+srv://Peppino:wHzr79JxnRUgK52@cluster0.zkagq.mongodb.net/?retryWrites=true&w=majority\")\n",
    "    mydb = mongo_client[\"maadb_tweets\"]\n",
    "    print(mydb.list_collection_names())\n",
    "\n",
    "coll_list = mydb.list_collection_names()\n",
    "#<editor-fold desc=\"Teeeest\">\n",
    "\n",
    "#</editor-fold>\n",
    "\n",
    "[print(i) for i in lex_resources_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}