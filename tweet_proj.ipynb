{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amato\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amato\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amato\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\amato\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amato\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# trovare le parole pi√π frequenti per ogni sentimento\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "# Se ne abbiamo voglia possiamo mettere le emoticons e gli emoji su file e per poi leggerli\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Set\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import pymongo\n",
    "import mariadb\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Punctuation and emojis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "#<editor-fold desc=\"Costants\">\n",
    "PUNCTUATION_MARKS = [',', '?', '!', '.', ';', ':', '\\\\', '/', '(', ')', '&', ' ', '_', '+', '=', '<', '>', '\"']\n",
    "\n",
    "EMOTICONS_POS = ['B-)', ':)', ':-)', \":')\", \":'-)\", ':D', ':-D', ':\\'-)', \":')\", ':o)', ':]', ':3', ':c)', ':>', '=]',\n",
    "                 '8)', '=)', ':}', ':^)', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', '=-3', '=3', 'B^D',\n",
    "                 ':-))', ':*', ':^*', '( \\'}{\\' )', '^^', '(^_^)', '^-^', \"^.^\", \"^3\\^\", \"\\^L\\^\", \";)\"]\n",
    "EMOTICONS_NEG = [':(', ':-(', \":'(\", \":'-(\", '>:[', ':-c', ':c', ':-<', ':<', ':-[', ':[', ':{', ':\\'-(', ':\\'(',\n",
    "                 ' _( ', ':\\'[', \"='(\", \"' [\", \"='[\", \":'-<\", \":' <\", \":'<\", \"=' <\", \"='<\", \"T_T\", \"T.T\", \"(T_T)\",\n",
    "                 \"y_y\", \"y.y\", \"(Y_Y)\", \";-;\", \";_;\", \";.;\", \":_:\", \"o .__. o\", \".-.\", \":/\", \";(\"]\n",
    "EMOTICONS = EMOTICONS_NEG + EMOTICONS_POS\n",
    "\n",
    "EMOJI_POS = [u'\\U0001F601', u'\\U0001F602', u'\\U0001F603', u'\\U0001F604', u'\\U0001F605', u'\\U0001F606', u'\\U0001F609',\n",
    "             u'\\U0001F60A', u'\\U0001F60B', u'\\U0001F60E', u'\\U0001F60D', u'\\U0001F618', u'\\U0001F617', u'\\U0001F619',\n",
    "             u'\\U0001F61A', u'\\U0000263A', u'\\U0001F642', u'\\U0001F917', u'\\U0001F607', u'\\U0001F60F', u'\\U0001F61C',\n",
    "             u'\\U0001F608', u'\\U0001F646', u'\\U0001F48F', u'\\U0001F44C', u'\\U0001F44F', u'\\U0001F48B', u'\\U0001F638',\n",
    "             u'\\U0001F639', u'\\U0001F63A', u'\\U0001F63B', u'\\U0001F63C', u'\\U0001F63D', u'\\U0001F192', u'\\U0001F197']\n",
    "EMOJI_NEG = [u'\\U0001F625', u'\\U0001F60C', u'\\U00002639', u'\\U0001F641', u'\\U0001F612', u'\\U0001F614', u'\\U0001F615',\n",
    "             u'\\U0001F616', u'\\U0001F632', u'\\U0001F61E', u'\\U0001F61F', u'\\U0001F622', u'\\U0001F62D', u'\\U0001F626',\n",
    "             u'\\U0001F627', u'\\U0001F628', u'\\U0001F631', u'\\U0001F621', u'\\U0001F620', u'\\U0001F64D', u'\\U0001F64E',\n",
    "             u'\\U0000270A', u'\\U0001F44A', u'\\U0001F494', u'\\U0001F4A2', u'\\U0001F5EF', u'\\U0001F63E', u'\\U0001F63F']\n",
    "OTHER_EMOJIS = [u'\\U0001F004', u'\\U0001F0CF', u'\\U0001F300', u'\\U0001F301', u'\\U0001F302', u'\\U0001F303', u'\\U0001F304',\n",
    "                u'\\U0001F305', u'\\U0001F306', u'\\U0001F307', u'\\U0001F309', u'\\U0001F30A', u'\\U0001F30B', u'\\U0001F30F',\n",
    "                u'\\U0001F313', u'\\U0001F315', u'\\U0001F31B', u'\\U0001F320', u'\\U0001F330', u'\\U0001F331', u'\\U0001F334',\n",
    "                u'\\U0001F337', u'\\U0001F338', u'\\U0001F339', u'\\U0001F33A', u'\\U0001F33B', u'\\U0001F33C', u'\\U0001F33D',\n",
    "                u'\\U0001F33E', u'\\U0001F33F', u'\\U0001F340', u'\\U0001F341', u'\\U0001F342', u'\\U0001F343', u'\\U0001F344',\n",
    "                u'\\U0001F345', u'\\U0001F346', u'\\U0001F347', u'\\U0001F348', u'\\U0001F349', u'\\U0001F34C', u'\\U0001F34D',\n",
    "                u'\\U0001F34E', u'\\U0001F34F', u'\\U0001F351', u'\\U0001F352', u'\\U0001F353', u'\\U0001F355', u'\\U0001F356',\n",
    "                u'\\U0001F357', u'\\U0001F358', u'\\U0001F35A', u'\\U0001F35B', u'\\U0001F35C', u'\\U0001F35D', u'\\U0001F35E',\n",
    "                u'\\U0001F35F', u'\\U0001F360', u'\\U0001F361', u'\\U0001F362', u'\\U0001F363', u'\\U0001F364', u'\\U0001F366',\n",
    "                u'\\U0001F367', u'\\U0001F368', u'\\U0001F369', u'\\U0001F36A', u'\\U0001F36B', u'\\U0001F36C', u'\\U0001F36D',\n",
    "                u'\\U0001F36E', u'\\U0001F36F', u'\\U0001F371', u'\\U0001F372', u'\\U0001F373', u'\\U0001F374', u'\\U0001F375',\n",
    "                u'\\U0001F376', u'\\U0001F377', u'\\U0001F378', u'\\U0001F37A', u'\\U0001F37B', u'\\U0001F380', u'\\U0001F381',\n",
    "                u'\\U0001F382', u'\\U0001F384', u'\\U0001F385', u'\\U0001F386', u'\\U0001F387', u'\\U0001F388', u'\\U0001F389',\n",
    "                u'\\U0001F38A', u'\\U0001F38B', u'\\U0001F38C', u'\\U0001F38D', u'\\U0001F38E', u'\\U0001F38F', u'\\U0001F390',\n",
    "                u'\\U0001F391', u'\\U0001F392', u'\\U0001F393', u'\\U0001F3A0', u'\\U0001F3A1', u'\\U0001F3A2', u'\\U0001F3A3',\n",
    "                u'\\U0001F3A4', u'\\U0001F3A5', u'\\U0001F3A6', u'\\U0001F3A7', u'\\U0001F3A8', u'\\U0001F3A9', u'\\U0001F3AA',\n",
    "                u'\\U0001F3AB', u'\\U0001F3AC', u'\\U0001F3AD', u'\\U0001F3AE', u'\\U0001F3AF', u'\\U0001F3B0', u'\\U0001F3B1',\n",
    "                u'\\U0001F3B2', u'\\U0001F3B3', u'\\U0001F3B4', u'\\U0001F3B5', u'\\U0001F3B6', u'\\U0001F3B7', u'\\U0001F3B8',\n",
    "                u'\\U0001F3B9', u'\\U0001F3BA', u'\\U0001F3BB', u'\\U0001F3BC', u'\\U0001F3BD', u'\\U0001F3BE', u'\\U0001F3BF',\n",
    "                u'\\U0001F3C0', u'\\U0001F3C1', u'\\U0001F3C2', u'\\U0001F3C3', u'\\U0001F3C4', u'\\U0001F3C6', u'\\U0001F3C8',\n",
    "                u'\\U0001F3CA', u'\\U0001F3E0', u'\\U0001F3E1', u'\\U0001F3E2', u'\\U0001F3E3', u'\\U0001F3E5', u'\\U0001F3E6',\n",
    "                u'\\U0001F3E7', u'\\U0001F3E8', u'\\U0001F3E9', u'\\U0001F3EA', u'\\U0001F3EB', u'\\U0001F3EC', u'\\U0001F3ED',\n",
    "                u'\\U0001F3EE', u'\\U0001F3EF', u'\\U0001F3F0', u'\\U0001F40C', u'\\U0001F40D', u'\\U0001F40E', u'\\U0001F411',\n",
    "                u'\\U0001F412', u'\\U0001F414', u'\\U0001F417', u'\\U0001F418', u'\\U0001F419', u'\\U0001F41A', u'\\U0001F41B',\n",
    "                u'\\U0001F41C', u'\\U0001F41D', u'\\U0001F41E', u'\\U0001F41F', u'\\U0001F420', u'\\U0001F421', u'\\U0001F422',\n",
    "                u'\\U0001F423', u'\\U0001F424', u'\\U0001F425', u'\\U0001F426', u'\\U0001F427', u'\\U0001F428', u'\\U0001F429',\n",
    "                u'\\U0001F42B', u'\\U0001F42C', u'\\U0001F42D', u'\\U0001F42E', u'\\U0001F42F', u'\\U0001F430', u'\\U0001F431',\n",
    "                u'\\U0001F432', u'\\U0001F433', u'\\U0001F434', u'\\U0001F435', u'\\U0001F436', u'\\U0001F437', u'\\U0001F438',\n",
    "                u'\\U0001F439', u'\\U0001F43A', u'\\U0001F43B', u'\\U0001F43C', u'\\U0001F43D', u'\\U0001F43E', u'\\U0001F440',\n",
    "                u'\\U0001F442', u'\\U0001F443', u'\\U0001F444', u'\\U0001F445', u'\\U0001F446', u'\\U0001F447', u'\\U0001F448',\n",
    "                u'\\U0001F449', u'\\U0001F44A', u'\\U0001F44B', u'\\U0001F44C', u'\\U0001F44D', u'\\U0001F44E', u'\\U0001F44F',\n",
    "                u'\\U0001F450', u'\\U0001F451', u'\\U0001F452', u'\\U0001F453', u'\\U0001F454', u'\\U0001F455', u'\\U0001F456',\n",
    "                u'\\U0001F457', u'\\U0001F458', u'\\U0001F459', u'\\U0001F45A', u'\\U0001F45B', u'\\U0001F45C', u'\\U0001F45D',\n",
    "                u'\\U0001F45E', u'\\U0001F45F', u'\\U0001F460', u'\\U0001F461', u'\\U0001F462', u'\\U0001F463', u'\\U0001F464',\n",
    "                u'\\U0001F466', u'\\U0001F467', u'\\U0001F468', u'\\U0001F469', u'\\U0001F46A', u'\\U0001F46B', u'\\U0001F46E',\n",
    "                u'\\U0001F46F', u'\\U0001F470', u'\\U0001F471', u'\\U0001F472', u'\\U0001F473', u'\\U0001F474', u'\\U0001F475',\n",
    "                u'\\U0001F476', u'\\U0001F477', u'\\U0001F478', u'\\U0001F479', u'\\U0001F47A', u'\\U0001F47B', u'\\U0001F47C',\n",
    "                u'\\U0001F47D', u'\\U0001F47E', u'\\U0001F47F', u'\\U0001F480', u'\\U0001F481', u'\\U0001F482', u'\\U0001F483',\n",
    "                u'\\U0001F484', u'\\U0001F485', u'\\U0001F486', u'\\U0001F487', u'\\U0001F488', u'\\U0001F489', u'\\U0001F48A',\n",
    "                u'\\U0001F48B', u'\\U0001F48C', u'\\U0001F48D', u'\\U0001F48E', u'\\U0001F48F', u'\\U0001F490', u'\\U0001F491',\n",
    "                u'\\U0001F492', u'\\U0001F493', u'\\U0001F494', u'\\U0001F495', u'\\U0001F496', u'\\U0001F497', u'\\U0001F498',\n",
    "                u'\\U0001F499', u'\\U0001F49A', u'\\U0001F49B', u'\\U0001F49C', u'\\U0001F49D', u'\\U0001F49E', u'\\U0001F49F',\n",
    "                u'\\U0001F4A0', u'\\U0001F4A1', u'\\U0001F4A2', u'\\U0001F4A3', u'\\U0001F4A4', u'\\U0001F4A5', u'\\U0001F4A6',\n",
    "                u'\\U0001F4A7', u'\\U0001F4A8', u'\\U0001F4A9', u'\\U0001F4AA', u'\\U0001F4AB', u'\\U0001F4AC', u'\\U0001F4AE',\n",
    "                u'\\U0001F4AF', u'\\U0001F4B0', u'\\U0001F4B1', u'\\U0001F4B2', u'\\U0001F4B3', u'\\U0001F4B4', u'\\U0001F4B5',\n",
    "                u'\\U0001F4B8', u'\\U0001F4B9', u'\\U0001F4BA', u'\\U0001F4BB', u'\\U0001F4BC', u'\\U0001F4BD', u'\\U0001F4BE',\n",
    "                u'\\U0001F4BF', u'\\U0001F4C0', u'\\U0001F4C1', u'\\U0001F4C2', u'\\U0001F4C3', u'\\U0001F4C4', u'\\U0001F4C5',\n",
    "                u'\\U0001F4C6', u'\\U0001F4C7', u'\\U0001F4C8', u'\\U0001F4C9', u'\\U0001F4CA', u'\\U0001F4CB', u'\\U0001F4CC',\n",
    "                u'\\U0001F4CD', u'\\U0001F4CE', u'\\U0001F4CF', u'\\U0001F4D0', u'\\U0001F4D1', u'\\U0001F4D2', u'\\U0001F4D3',\n",
    "                u'\\U0001F4D4', u'\\U0001F4D5', u'\\U0001F4D6', u'\\U0001F4D7', u'\\U0001F4D8', u'\\U0001F4D9', u'\\U0001F4DA',\n",
    "                u'\\U0001F4DB', u'\\U0001F4DC', u'\\U0001F4DD', u'\\U0001F4DE', u'\\U0001F4DF', u'\\U0001F4E0', u'\\U0001F4E1',\n",
    "                u'\\U0001F4E2', u'\\U0001F4E3', u'\\U0001F4E4', u'\\U0001F4E5', u'\\U0001F4E6', u'\\U0001F4E7', u'\\U0001F4E8',\n",
    "                u'\\U0001F4E9', u'\\U0001F4EA', u'\\U0001F4EB', u'\\U0001F4EE', u'\\U0001F4F0', u'\\U0001F4F1', u'\\U0001F4F2',\n",
    "                u'\\U0001F4F3', u'\\U0001F4F4', u'\\U0001F4F6', u'\\U0001F4F7', u'\\U0001F4F9', u'\\U0001F4FA', u'\\U0001F4FB',\n",
    "                u'\\U0001F4FC', u'\\U0001F503', u'\\U0001F50A', u'\\U0001F50B', u'\\U0001F50C', u'\\U0001F50D', u'\\U0001F50E',\n",
    "                u'\\U0001F50F', u'\\U0001F510', u'\\U0001F511', u'\\U0001F512', u'\\U0001F513', u'\\U0001F514', u'\\U0001F516',\n",
    "                u'\\U0001F517', u'\\U0001F518', u'\\U0001F519', u'\\U0001F51A', u'\\U0001F51B', u'\\U0001F51C', u'\\U0001F51D',\n",
    "                u'\\U0001F51E', u'\\U0001F51F', u'\\U0001F520', u'\\U0001F521', u'\\U0001F522', u'\\U0001F523', u'\\U0001F524',\n",
    "                u'\\U0001F525', u'\\U0001F526', u'\\U0001F527', u'\\U0001F528', u'\\U0001F529', u'\\U0001F52A', u'\\U0001F52B',\n",
    "                u'\\U0001F52E', u'\\U0001F52F', u'\\U0001F530', u'\\U0001F531', u'\\U0001F532', u'\\U0001F533', u'\\U0001F534',\n",
    "                u'\\U0001F535', u'\\U0001F536', u'\\U0001F537', u'\\U0001F538', u'\\U0001F539', u'\\U0001F53A', u'\\U0001F53B',\n",
    "                u'\\U0001F53C', u'\\U0001F53D', u'\\U0001F550', u'\\U0001F551', u'\\U0001F552', u'\\U0001F553', u'\\U0001F554',\n",
    "                u'\\U0001F555', u'\\U0001F556', u'\\U0001F557', u'\\U0001F558', u'\\U0001F559', u'\\U0001F55A', u'\\U0001F55B',\n",
    "                u'\\U0001F5FB', u'\\U0001F5FC', u'\\U0001F5FD', u'\\U0001F5FE', u'\\U0001F5FF', u'\\U0001F601', u'\\U0001F602',\n",
    "                u'\\U0001F603', u'\\U0001F604', u'\\U0001F605', u'\\U0001F606', u'\\U0001F609', u'\\U0001F60F', u'\\U0001F612',\n",
    "                u'\\U0001F613', u'\\U0001F61C', u'\\U0001F61D', u'\\U0001F61E', u'\\U0001F620', u'\\U0001F621', u'\\U0001F622',\n",
    "                u'\\U0001F623', u'\\U0001F624', u'\\U0001F625', u'\\U0001F628', u'\\U0001F629', u'\\U0001F62A', u'\\U0001F62B',\n",
    "                u'\\U0001F630', u'\\U0001F631', u'\\U0001F632', u'\\U0001F633', u'\\U0001F635', u'\\U0001F637', u'\\U0001F638',\n",
    "                u'\\U0001F639', u'\\U0001F63A', u'\\U0001F63B', u'\\U0001F63C', u'\\U0001F63D', u'\\U0001F63E', u'\\U0001F63F',\n",
    "                u'\\U0001F640', u'\\U0001F645', u'\\U0001F646', u'\\U0001F647', u'\\U0001F648', u'\\U0001F649', u'\\U0001F64A',\n",
    "                u'\\U0001F64B', u'\\U0001F64C', u'\\U0001F64E', u'\\U0001F64F', u'\\U0001F64F']\n",
    "# AdditionalEmoji=[u'\\U+203C',u'\\U+2049', u'\\U+231A',u'\\U+231B',u'\\U+2600',u'\\U+2601',u'\\U+260E',u'\\U+2611',u'\\U+2614',u'\\U+2615',u'\\U+261D',u'\\U+2648',u'\\U+2648',u'\\U+2649',u'\\U+264A',u'\\U+264B',u'\\U+264C',u'\\U+264D',u'\\U+264E',u'\\U+264F',u'\\U+2650',u'\\U+2651',u'\\U+2652',u'\\U+2653',u'\\U+2660',u'\\U+2663',u'\\U+2665',u'\\U+2666',u'\\U+2668',u'\\U+267B',u'\\U+267F',u'\\U+2693',u'\\U+26A0',u'\\U+26A1',u'\\U+26AA',u'\\U+26AB',u'\\U+26BD',u'\\U+26BE',u'\\U+26C4',u'\\U+26C5',u'\\U+26CE',u'\\U+26D4',u'\\U+26EA',u'\\U+26F2',u'\\U+26F3',u'\\U+26F5',u'\\U+26FA',u'\\U+26FD',u'\\U+2934',u'\\U+2935',u'\\U+2934',u'\\U+2B05',u'\\U+2B06',u'\\U+2B07',u'\\U+2B50',u'\\U+2B55',u'\\U+2B50']\n",
    "EMOJIS = EMOJI_POS + EMOJI_NEG + OTHER_EMOJIS\n",
    "\n",
    "SLANGS = {'afaik': 'as far as i know', 'afk': 'away from keyboard', 'asap': 'as soon as possible',\n",
    "          'atk': 'at the keyboard', 'atm': 'at the moment', 'a3': 'anytime, anywhere, anyplace',\n",
    "          'bak': 'back at keyboard', 'bbl': 'be back later', 'bbs': 'be back soon', 'bfn/b4n': 'bye for now',\n",
    "          'brb': 'be right back', 'brt': 'be right there', 'btw': 'by the way', 'b4n': 'bye for now', 'cu': 'see you',\n",
    "          'cul8r': 'see you later', 'cya': 'see you', 'faq': 'frequently asked questions', 'fc': 'fingers crossed',\n",
    "          'fwiw': 'for what it\\'s worth', 'fyi': 'for your information', 'gal': 'get a life', 'gg': 'good game',\n",
    "          'gmta': 'great minds think alike', 'gr8': 'great!', 'g9': 'genius', 'ic': 'i see', 'icq': 'i seek you',\n",
    "          'ilu': 'ilu: i love you', 'imho': 'in my honest opinion', 'imo': 'in my opinion', 'iow': 'in other words',\n",
    "          'irl': 'in real life', 'kiss': 'keep it simple, stupid', 'ldr': 'long distance relationship',\n",
    "          'lmao': 'laugh my a.. off', 'lol': 'laughing out loud', 'ltns': 'long time no see', 'l8r': 'later',\n",
    "          'mte': 'my thoughts exactly', 'm8': 'mate', 'nrn': 'no reply necessary', 'oic': 'oh i see',\n",
    "          'pita': 'pain in the a..', 'prt': 'party', 'prw': 'parents are watching', 'qpsa?': 'que pasa?',\n",
    "          'rofl': 'rolling on the floor laughing', 'roflol': 'rolling on the floor laughing out loud',\n",
    "          'rotflmao': 'rolling on the floor laughing my a.. off', 'sk8': 'skate', 'stats': 'your sex and age',\n",
    "          'asl': 'age, sex, location', 'thx': 'thank you', 'ttfn': 'ta-ta for now!', 'ttyl': 'talk to you later',\n",
    "          ' u': ' you', 'u ': 'you ', 'u.': 'you.', 'u2': 'you too', 'u4e': 'yours for ever', 'wb': 'welcome back', 'wtf': 'what the f...',\n",
    "          'wtg': 'way to go!', 'wuf': 'where are you from?', 'w8': 'wait...', '7k': 'sick:-d laugher'}\n",
    "#</editor-fold>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Directories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "lex_resources_directory = \"resources/test/lex_res\"\n",
    "tweets_directory = \"resources/test/tweets/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Class used to store information about each tweet: its sentiment e how many words of which sentiment are in that tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "class TweetInfo:\n",
    "    sentiment: str\n",
    "    sentiment_occurrences: Dict[str, int]\n",
    "\n",
    "    def __init__(self, sentiment: str, sentiments_for_words: List[str]):\n",
    "        self.sentiment = sentiment\n",
    "\n",
    "        self.sentiment_occurrences = {}\n",
    "        for sentiment_for_words in sentiments_for_words:\n",
    "            # initiate every word sentiment occurrence to 0\n",
    "            self.sentiment_occurrences[sentiment_for_words] = 0\n",
    "\n",
    "    def increase_sentiment_counter(self, sentiment: str):\n",
    "        self.sentiment_occurrences[sentiment] = self.sentiment_occurrences.get(sentiment) + 1\n",
    "\n",
    "    def print_tweet_info(self):\n",
    "        print(\"tweet sentiment: \", self.sentiment)\n",
    "        for word_sentiment in self.sentiment_occurrences:\n",
    "            print(\"\\t word sentiment: \", word_sentiment)\n",
    "            print(\"\\t occurrences: \", self.sentiment_occurrences[word_sentiment])\n",
    "\n",
    "    def get_sentiment(self) -> str:\n",
    "        return sentiment\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class LexicalResource:\n",
    "    filename: str\n",
    "    sentiment: str\n",
    "    word_list: List[str]\n",
    "\n",
    "    def __init__(self, filename: str, sentiment: str):\n",
    "        self.filename = filename\n",
    "        self.sentiment = sentiment\n",
    "        self.word_list = []\n",
    "\n",
    "    def __str__(self):\n",
    "        lex_res_string = \"LexicalResource: \" + self.filename + \\\n",
    "                         \"\\n\\t sentiment: \" + self.sentiment + \\\n",
    "                         \"\\n\\t wordlist: \" + self.word_list.__str__()\n",
    "        return lex_res_string\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        if not '_' in word:\n",
    "            self.word_list.append(word)\n",
    "\n",
    "    def add_word_list(self, word_list: List[str]):\n",
    "        for word in word_list:\n",
    "            if not '_' in word:\n",
    "                self.word_list.append(word)\n",
    "\n",
    "    def get_number_of_words(self) -> int:\n",
    "        return len(self.word_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    index: int\n",
    "    text: str\n",
    "    hashtags: List[str]\n",
    "    emojis: List[str]\n",
    "    emoticons: List[str]\n",
    "    tokens: List[str]\n",
    "    words: List[str]\n",
    "    pos_tags: Dict[str, str]\n",
    "    tweet_stem_count: TweetInfo\n",
    "    word_frequency: Dict[str, int] = {}\n",
    "    sentiment: str\n",
    "\n",
    "    def __init__(self, tweet_raw: str, index: int, sentiment: str):\n",
    "        self.index = index\n",
    "        self.text = tweet_raw\n",
    "        self.anonymize()\n",
    "        self.read_hashtags()\n",
    "        self.read_emojis()\n",
    "        self.read_emoticons()\n",
    "        self.to_lower()\n",
    "        self.tokenize()\n",
    "        self.process_slangs()\n",
    "        self.pos_tagging()\n",
    "        self.remove_punctuation()\n",
    "        self.lemming()\n",
    "        self.remove_stop_words()\n",
    "        self.count_words_frequency()\n",
    "        self.sentiment = sentiment\n",
    "\n",
    "    # TODO fai il to string al posto del print tweet\n",
    "    def __str__(self):\n",
    "        tweet_string = \"Tweet\\n\"\n",
    "        tweet_string = tweet_string + \"\\ttweet raw: \" + self.text\n",
    "        tweet_string = tweet_string + \"\\tpos tags: \" + json.dumps(self.pos_tags)\n",
    "        tweet_string = tweet_string + \"\\n\"\n",
    "        return tweet_string\n",
    "\n",
    "    def read_hashtags(self) -> None:\n",
    "        self.hashtags = re.findall(r\"#(\\w+)\", self.text)\n",
    "\n",
    "    def read_emojis(self) -> None:\n",
    "        self.emojis = get_elems_from_text_if_in_list(self.text, EMOJIS)\n",
    "\n",
    "    def read_emoticons(self) -> None:\n",
    "        self.emoticons = get_elems_from_text_if_in_list(self.text, EMOTICONS)\n",
    "\n",
    "    def anonymize(self) -> None:\n",
    "        self.text = self.text.replace(\"USERNAME\", \"\").replace(\"URL\", \"\")\n",
    "\n",
    "    def to_lower(self) -> None:\n",
    "        self.text = self.text.lower()\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        # Questa funzione mi sa che non andava bene, poi vediamo\n",
    "        self.tokens = nltk.word_tokenize(self.text)\n",
    "        # self.tokens = sent_tokenize(self.text)\n",
    "\n",
    "    def pos_tagging(self) -> None:\n",
    "        pos_tag_list = nltk.pos_tag(self.tokens)\n",
    "\n",
    "        # get keys of pos tag list\n",
    "        pos_tag_keys = [pos_tag[0] for pos_tag in pos_tag_list]\n",
    "        # get values of pos tag list\n",
    "        pos_tag_values = [pos_tag[1] for pos_tag in pos_tag_list]\n",
    "        # create dictionary\n",
    "        pos_tag_dict: Dict[str, str] = {pos_tag_keys[i]: pos_tag_values[i] for i in range(len(pos_tag_keys))}\n",
    "\n",
    "        self.pos_tags = pos_tag_dict\n",
    "\n",
    "    def remove_punctuation(self) -> None: # rimuove anche emoticons, giusto?\n",
    "        # Removes every character besides lower and uppercase letters, numbers and spaces\n",
    "        # self.text = re.sub(r'[^a-zA-Z0-9 ]', '', self.text)\n",
    "        for tag_key in list(self.pos_tags.keys()):\n",
    "            if tag_key in PUNCTUATION_MARKS:\n",
    "                # tag key is a punctuation mark, so remove from pos tagging list\n",
    "                del self.pos_tags[tag_key]\n",
    "\n",
    "    def print_tweet(self) -> None:\n",
    "        print(\"tweet raw: \", self.text)\n",
    "        print(\"pos tagging: \", self.pos_tags)\n",
    "        # print(\"\\n\\ttokens \", self.tokens)\n",
    "        # print(\"\\n\\thashtag_list \", self.hashtags)\n",
    "        # print(\"\\n\\temoji_list \", self.emojis)\n",
    "        # print(\"\\n\\temoticon_list \", self.emoticons)\n",
    "        # print(\"\\n\\twords_list \", self.get_words())\n",
    "\n",
    "    def lemming(self) -> None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tweet_words_lemmatized: List[str] = []\n",
    "\n",
    "        tweet_words = self.get_words()\n",
    "        for word in tweet_words:\n",
    "            tweet_words_lemmatized.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    def remove_stop_words(self) -> None:\n",
    "\n",
    "        # TODO implement this\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        tokens = self.tokens\n",
    "        for w in tokens:\n",
    "            if w in stop_words:\n",
    "                tokens.remove(w)\n",
    "        self.words = tokens\n",
    "\n",
    "        pos_tags = self.pos_tags # removing stop words from pos tags\n",
    "        for w in pos_tags:\n",
    "            if w in stop_words:\n",
    "                self.pos_tags = removekey(self.pos_tags, w)\n",
    "\n",
    "\n",
    "    def process_slangs(self) -> None:\n",
    "        for slang in SLANGS:\n",
    "            # replaces the slang with the extension for every slang in the text\n",
    "            self.text = self.text.replace(slang, SLANGS[slang])\n",
    "\n",
    "    def count_words_frequency(self):\n",
    "        words = self.words\n",
    "        for word in words:\n",
    "            self.word_frequency[word] = words.count(word)\n",
    "\n",
    "    # Support functions\n",
    "\n",
    "    def get_words(self) -> List[str]:\n",
    "        return self.text.split()\n",
    "\n",
    "def removekey(d, key):\n",
    "    r = dict(d)\n",
    "    del r[key]\n",
    "    return r"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def get_elems_from_text_if_in_list(text: str, list: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return list of substrings of text that appear in list\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in list:\n",
    "            matches.append(word)\n",
    "\n",
    "    return matches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Read files in directory\n",
    "General function to read text files from a directory and merge them"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "lex_resources_list: List[LexicalResource] = []\n",
    "\n",
    "def read_texts_in_directory(directory_path: str, sentiment: str) -> List[str]:\n",
    "    files_text_list: List[str] = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            file = open(file=file_path, encoding=\"utf8\")\n",
    "            file_text = file.read().split() # list of words for a single lex resource of a sentiment\n",
    "            files_text_list = files_text_list + file_text\n",
    "\n",
    "            lex_res: LexicalResource = LexicalResource(filename, sentiment)\n",
    "            lex_res.add_word_list(file_text)\n",
    "            global lex_resources_list\n",
    "            lex_resources_list.append(lex_res)\n",
    "\n",
    "    # print(len(lex_resources_list))\n",
    "    # [print(i) for i in lex_resources_list]\n",
    "    return files_text_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Read lexical resources for a sentiment\n",
    "Function which reads all the lexical resources for a sentiment\n",
    "The directory containing all lexical resources for that sentiment is passed as parameter\n",
    "Returns a set of the words in all the lexical resources of a sentiment\n",
    "### forse creare per ogni lex res di OGNI sentimento un dizionario diverso? Bisogna vedere come caricare i dati su db, bisogna caricare ogni lex res diversa di ogni sentimento sul db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def read_lex_resources_sentiment(sentiment_lex_resources_directory: str, sentiment: str) -> Set[str]:\n",
    "    resource_words: Set[str] = set()\n",
    "    resources_text: List[str] = read_texts_in_directory(sentiment_lex_resources_directory, sentiment)\n",
    "    for word in resources_text:\n",
    "        if not '_' in word:\n",
    "            resource_words.add(word)\n",
    "    #print(sentiment, \"\\n\", resource_words, \"\\n\\n\")\n",
    "    return resource_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read all lexical resources\n",
    "\n",
    "Reads all the lexical resources and returns a dictionary of word to sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bu': 'Anger', 'no': 'Anger', 'bubu': 'Anger', 'pensa': 'Joy', 'angry': 'Anger', 'bbu': 'Anger', 'ya': 'Anger', 'boella': 'Joy', 'privacy': 'Joy', 'yes': 'Joy'}\n"
     ]
    }
   ],
   "source": [
    "sentiment_lex_resources: Dict[str, str] = {}\n",
    "\n",
    "for resources_path, sentiments, _ in os.walk(lex_resources_directory):\n",
    "    # The folders inside the lexical resources folder are named after a sentiment (Ex. Anger, Joy), each of them contain some files and each of them is a list of words that are associated with that sentiment\n",
    "    \"\"\"WARN Ad ogni ciclo ci sarebbe per forza un singolo sentiment dato che cicla sulla directory delle directory di lex res?\"\"\"\n",
    "    for sentiment in sentiments:\n",
    "        # iterate each folder (one for sentiment)\n",
    "        resources_sentiment_path = os.path.join(resources_path, sentiment)\n",
    "        sentiment_words_set: Set[str] = read_lex_resources_sentiment(resources_sentiment_path, sentiment)\n",
    "\n",
    "        # read the files containing lists of words, and return a set of all the words in those files\n",
    "        for sentiment_word in sentiment_words_set:\n",
    "            # associate each word of the set to the corresponding sentiment\n",
    "            sentiment_lex_resources[sentiment_word] = sentiment\n",
    "\n",
    "lex_word_to_sentiment = sentiment_lex_resources\n",
    "print(lex_word_to_sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tweet reading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (function) Reads a file and converts the text to tweets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def read_tweet_file(file_path_string: str, sentiment: str) -> List[Tweet]:\n",
    "    \"\"\"\n",
    "    Reads a file and converts the text to tweets\n",
    "    :param file_path_string: string of the path to the file\n",
    "    \"\"\"\n",
    "\n",
    "    # tweets read from file\n",
    "    tweets_read: List[Tweet] = []\n",
    "\n",
    "    tweets_file = open(file=file_path_string, encoding=\"utf8\")\n",
    "    tweets_text: List[str] = tweets_file.readlines()\n",
    "\n",
    "    # For each tweet text create a Tweet object\n",
    "    i = 0\n",
    "    for tweet_text in tweets_text:\n",
    "        i = i + 1\n",
    "        new_tweet = Tweet(tweet_text, i, sentiment)\n",
    "        tweets_read.append(new_tweet)\n",
    "\n",
    "    return tweets_read"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get list of sentiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "sentiments: List[str] = [sentiment for sentiment in os.listdir(lex_resources_directory)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read tweets folder and load Tweet Info for stem counting\n",
    "The tweets folder contains for each sentiment a file containing tweets of that sentiment. Each file is scanned and for each tweet a TweetInfo object is created in order to maintain the count of how many word of which sentiments are in it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ma associare il TweetInfo al tweet senza fare un altro dict?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def get_tweet_sentiment_from_file_name(file_name: str):\n",
    "    extension_removed = file_name.split(\".\")[0]\n",
    "    sentiment = extension_removed.split(\"_\")[-2]\n",
    "    return sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "tweets_to_info: Dict[Tweet, TweetInfo] = {}\n",
    "for tweets_sentiments_directory, _, tweets_sentiments_filenames in os.walk(tweets_directory):\n",
    "    # print(tweets_sentiments_directory)\n",
    "    # print(tweets_sentiments_files)\n",
    "\n",
    "    for tweets_sentiment_filename in tweets_sentiments_filenames:\n",
    "        # print(tweets_sentiment_file)\n",
    "        tweets_sentiment_filepath = os.path.join(tweets_sentiments_directory, tweets_sentiment_filename)\n",
    "        sentiment = get_tweet_sentiment_from_file_name(tweets_sentiment_filename)\n",
    "        tweets_for_sentiment: List[Tweet] = read_tweet_file(tweets_sentiment_filepath, sentiment)\n",
    "        #print(\"Tweets for sentiment: \", sentiment, \"\\n\")\n",
    "        for tweet in tweets_for_sentiment:\n",
    "            tweet_info: TweetInfo = TweetInfo(sentiment, sentiments)\n",
    "            tweet.tweet_stem_count = TweetInfo\n",
    "            tweets_to_info[tweet] = tweet_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stem counting\n",
    "For each tweet and each word of them is checked the sentiment and increased the counter for that sentiment in the TweetInfo object associated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "for tweet in tweets_to_info:\n",
    "    tweet_info = tweets_to_info[tweet]\n",
    "    tweet_words: List[str] = tweet.get_words()\n",
    "\n",
    "    for word in tweet_words:\n",
    "        if word in lex_word_to_sentiment:\n",
    "            # get the sentiment for the word and increase sentiment counter by 1\n",
    "            sentiment = lex_word_to_sentiment[word]\n",
    "            tweet_info.increase_sentiment_counter(sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test print"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def print_tweets():\n",
    "    for tweet in tweets_to_info.keys():\n",
    "        info = tweets_to_info[tweet]\n",
    "        print(tweet)\n",
    "        print(\"sentiment: \" + info.sentiment)\n",
    "        print(\"sentiment occurrences: \")\n",
    "        print(info.sentiment_occurrences)\n",
    "        print(\"---\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet\n",
      "\ttweet raw:  yes the cillo is very chill smoking on the balcony ;)\n",
      "\tpos tags: {\"yes\": \"RB\", \"cillo\": \"NN\", \"chill\": \"JJ\", \"smoking\": \"VBG\", \"balcony\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 1}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: wow i'm having fun doing the smoking\n",
      "\tpos tags: {\"wow\": \"NN\", \"'m\": \"VBP\", \"fun\": \"NN\", \"smoking\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: yea  yea\n",
      "\tpos tags: {\"yea\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: boss\tpos tags: {\"boss\": \"NN\"}\n",
      "\n",
      "sentiment: cillo\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: angry pensa is angry sad banana no\n",
      "\tpos tags: {\"angry\": \"JJ\", \"pensa\": \"NN\", \"sad\": \"JJ\", \"banana\": \"NN\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 3, 'Joy': 1}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw: angry boella no pensa kill ;( yoyou ah rip bu\n",
      "\tpos tags: {\"angry\": \"JJ\", \"boella\": \"NN\", \"pensa\": \"NN\", \"kill\": \"NN\", \"ah\": \"VBP\", \"rip\": \"JJ\", \"bu\": \"NN\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 3, 'Joy': 2}\n",
      "---\n",
      "Tweet\n",
      "\ttweet raw:  know what she ain't üòí don't even need to say it !\tpos tags: {\"know\": \"VB\", \"ai\": \"VBP\", \"n't\": \"RB\", \"\\ud83d\\ude12\": \"NNP\", \"even\": \"RB\", \"need\": \"VB\", \"say\": \"VB\"}\n",
      "\n",
      "sentiment: pensa\n",
      "sentiment occurrences: \n",
      "{'Anger': 0, 'Joy': 0}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print_tweets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MongoDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tweets', 'LexResourcesWords', 'LexResources']\n",
      "LexicalResource: angry\n",
      "\t sentiment: Anger\n",
      "\t wordlist: ['no', 'pensa', 'angry', 'bu']\n",
      "LexicalResource: pensa_angry.txt\n",
      "\t sentiment: Anger\n",
      "\t wordlist: ['bbu', 'bubu', 'ya', 'bu']\n",
      "LexicalResource: happy\n",
      "\t sentiment: Joy\n",
      "\t wordlist: ['yes', 'boella', 'privacy', 'pensa']\n"
     ]
    },
    {
     "data": {
      "text/plain": "[None, None, None]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_connected = False\n",
    "if not already_connected:\n",
    "    mongo_client = pymongo.MongoClient(\"mongodb+srv://Peppino:wHzr79JxnRUgK52@cluster0.zkagq.mongodb.net/?retryWrites=true&w=majority\")\n",
    "    mydb = mongo_client[\"maadb_tweets\"]\n",
    "    print(mydb.list_collection_names())\n",
    "\n",
    "coll_list = mydb.list_collection_names()\n",
    "\n",
    "\n",
    "[print(i) for i in lex_resources_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check in which resources each word is contained\n",
    "Creates a dictionary <word, lex_res_list> to map each word with the lexical resources which contains the word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bu': ['angry', 'pensa_angry.txt'], 'no': ['angry'], 'bubu': ['pensa_angry.txt'], 'pensa': ['angry', 'happy'], 'angry': ['angry'], 'bbu': ['pensa_angry.txt'], 'ya': ['pensa_angry.txt'], 'boella': ['happy'], 'privacy': ['happy'], 'yes': ['happy']}\n"
     ]
    }
   ],
   "source": [
    "map_word_lex_res: Dict[str, List[str]] = {}\n",
    "\n",
    "for word in lex_word_to_sentiment:\n",
    "    for lex_res in lex_resources_list:\n",
    "        if word in lex_res.word_list:\n",
    "            if map_word_lex_res.get(word) is None:\n",
    "                map_word_lex_res[word] = [lex_res.filename]\n",
    "            else:\n",
    "                map_word_lex_res[word].append(lex_res.filename)\n",
    "\n",
    "print(map_word_lex_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flags to manage queries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "delete_lex_res = True\n",
    "insert_lex_res = False\n",
    "\n",
    "delete_lex_res_words = True\n",
    "insert_lex_res_words = False\n",
    "\n",
    "delete_tweets = True\n",
    "insert_tweets = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete Lexical Resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "db_lex_res_collection = mydb[\"LexResources\"]\n",
    "\n",
    "if delete_lex_res:\n",
    "    db_lex_res_collection.delete_many({})\n",
    "\n",
    "if insert_lex_res:\n",
    "    for lex_res in lex_resources_list:\n",
    "        to_upload = {\"_id\" : lex_res.filename,\n",
    "                     \"sentiment\": lex_res.sentiment,\n",
    "                     \"totNumberWords\" : lex_res.get_number_of_words()}\n",
    "        inserted_lex_res = db_lex_res_collection.insert_one(to_upload)\n",
    "        print(inserted_lex_res.inserted_id)\n",
    "        # until here inserted lexical resources basic information in LexRes collection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete words of lexical resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " {}\n"
     ]
    }
   ],
   "source": [
    "map_lex_word_db_id: Dict[str, int] = {}\n",
    "db_lex_res_words_collection = mydb[\"LexResourcesWords\"]\n",
    "\n",
    "if delete_lex_res_words:\n",
    "    db_lex_res_words_collection.delete_many({})\n",
    "\n",
    "if insert_lex_res_words:\n",
    "    # for each word in all the lexical resources insert in LexResWords the word and a\n",
    "    # list of pairs <$ref, $id> to track in which LexRes the word is contained\n",
    "    for word in lex_word_to_sentiment:\n",
    "        list_lex_res = map_word_lex_res[word] # list of lexical resources in which the word is contained\n",
    "        resources = [] # list of pairs to insert in LexResWords\n",
    "\n",
    "        for res in list_lex_res: # populate list adding, one at a time, the lexical resources in which the word is contained\n",
    "            resources.append({\"$ref\": \"LexResources\", \"$id\": res})\n",
    "\n",
    "        word_to_upload = {\"lemma\" : word,\n",
    "                          \"resources\" : resources}\n",
    "        inserted_lex_res_word = db_lex_res_words_collection.insert_one(word_to_upload)\n",
    "        map_lex_word_db_id[word] = inserted_lex_res_word.inserted_id # save object id to use it later to reference resources words from tweet words\n",
    "        print(word_to_upload)\n",
    "\n",
    "print(\"\\n\\n\", map_lex_word_db_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert/delete tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "db_tweets_collection = mydb[\"Tweets\"]\n",
    "\n",
    "if delete_tweets:\n",
    "    db_tweets_collection.delete_many({})\n",
    "\n",
    "if insert_tweets:\n",
    "    for tweet in tweets_to_info:\n",
    "        tweet_words_upload = []\n",
    "        for word in tweet.pos_tags:\n",
    "            if map_lex_word_db_id.get(word) is None:\n",
    "                # Decide what to do with words that do not have a lexical resource associated, we could think about associating it to a resource or some other strategy.\n",
    "                tweet_words_upload.append({\n",
    "                    \"lemma\": word,\n",
    "                    \"POS\": tweet.pos_tags[word],\n",
    "                    \"freq\": tweet.word_frequency[word],\n",
    "                    \"in_lex_resources\" : \"None\"})\n",
    "            else:\n",
    "                tweet_words_upload.append({\n",
    "                    \"lemma\": word,\n",
    "                    \"POS\": tweet.pos_tags[word],\n",
    "                    \"freq\": tweet.word_frequency[word],\n",
    "                    \"in_lex_resources\" : {\"$ref\": \"LexResourcesWords\", \"$id\": map_lex_word_db_id[word]}})\n",
    "\n",
    "        tweet_to_upload = {\n",
    "            \"sentiment\": tweet.sentiment,\n",
    "            \"index\": tweet.index,\n",
    "            \"words\" : tweet_words_upload,\n",
    "            \"hashtags\" : tweet.hashtags,\n",
    "            \"emojis\" : tweet.emojis,\n",
    "            \"emoticons\" : tweet.emoticons}\n",
    "\n",
    "        inserted_tweets = db_tweets_collection.insert_one(tweet_to_upload)\n",
    "\n",
    "        print(tweet)\n",
    "        print(tweet_to_upload, \"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "db_tweets_collection = mydb[\"Tweets\"]\n",
    "\n",
    "if delete_tweets:\n",
    "    db_tweets_collection.delete_many({})\n",
    "\n",
    "if insert_tweets:\n",
    "    for tweet in tweets_to_info:\n",
    "        tweet_words_upload = []\n",
    "        for word in tweet.pos_tags:\n",
    "            if map_lex_word_db_id.get(word) is None:\n",
    "                # Decide what to do with words that do not have a lexical resource associated, we could think about associating it to a resource or some other strategy.\n",
    "                tweet_words_upload.append({\n",
    "                    \"lemma\": word,\n",
    "                    \"POS\": tweet.pos_tags[word],\n",
    "                    \"freq\": tweet.word_frequency[word],\n",
    "                    \"in_lex_resources\" : \"None\"})\n",
    "            else:\n",
    "                tweet_words_upload.append({\n",
    "                    \"lemma\": word,\n",
    "                    \"POS\": tweet.pos_tags[word],\n",
    "                    \"freq\": tweet.word_frequency[word],\n",
    "                    \"in_lex_resources\" : {\"$ref\": \"LexResourcesWords\", \"$id\": map_lex_word_db_id[word]}})\n",
    "\n",
    "        tweet_to_upload = {\n",
    "            \"sentiment\": tweet.sentiment,\n",
    "            \"index\": tweet.index,\n",
    "            \"words\" : tweet_words_upload,\n",
    "            \"hashtags\" : tweet.hashtags,\n",
    "            \"emojis\" : tweet.emojis,\n",
    "            \"emoticons\" : tweet.emoticons}\n",
    "\n",
    "        inserted_tweets = db_tweets_collection.insert_one(tweet_to_upload)\n",
    "\n",
    "        print(tweet)\n",
    "        print(tweet_to_upload, \"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MariaDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MariaDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error connecting to MariaDB Platform: Access denied for user 'root'@'localhost' (using password: YES)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amato\\AppData\\Local\\Temp/ipykernel_13628/985685160.py\", line 5, in <module>\n",
      "    conn = mariadb.connect(\n",
      "mariadb.OperationalError: Access denied for user 'root'@'localhost' (using password: YES)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\amato\\AppData\\Local\\Temp/ipykernel_13628/985685160.py\", line 14, in <module>\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperationalError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_13628/985685160.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m         conn = mariadb.connect(\n\u001B[0m\u001B[0;32m      6\u001B[0m             \u001B[0muser\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"root\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOperationalError\u001B[0m: Access denied for user 'root'@'localhost' (using password: YES)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mSystemExit\u001B[0m                                Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_13628/985685160.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Error connecting to MariaDB Platform: {e}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m         \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mSystemExit\u001B[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[1;32mc:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   2055\u001B[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001B[0;32m   2056\u001B[0m                            'the full traceback.\\n']\n\u001B[1;32m-> 2057\u001B[1;33m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001B[0m\u001B[0;32m   2058\u001B[0m                                                                      value))\n\u001B[0;32m   2059\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mget_exception_only\u001B[1;34m(self, etype, value)\u001B[0m\n\u001B[0;32m    752\u001B[0m         \u001B[0mvalue\u001B[0m \u001B[1;33m:\u001B[0m \u001B[0mexception\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    753\u001B[0m         \"\"\"\n\u001B[1;32m--> 754\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mListTB\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstructured_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    755\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    756\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mshow_exception_only\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, evalue, etb, tb_offset, context)\u001B[0m\n\u001B[0;32m    627\u001B[0m             \u001B[0mchained_exceptions_tb_offset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    628\u001B[0m             out_list = (\n\u001B[1;32m--> 629\u001B[1;33m                 self.structured_traceback(\n\u001B[0m\u001B[0;32m    630\u001B[0m                     \u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0metb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mchained_exc_ids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    631\u001B[0m                     chained_exceptions_tb_offset, context)\n",
      "\u001B[1;32mc:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1365\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1366\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1367\u001B[1;33m         return FormattedTB.structured_traceback(\n\u001B[0m\u001B[0;32m   1368\u001B[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[0;32m   1369\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1265\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mverbose_modes\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1266\u001B[0m             \u001B[1;31m# Verbose modes need a full traceback\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1267\u001B[1;33m             return VerboseTB.structured_traceback(\n\u001B[0m\u001B[0;32m   1268\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1269\u001B[0m             )\n",
      "\u001B[1;32mc:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1122\u001B[0m         \u001B[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1123\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1124\u001B[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0m\u001B[0;32m   1125\u001B[0m                                                                tb_offset)\n\u001B[0;32m   1126\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[0;32m   1080\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1081\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1082\u001B[1;33m         \u001B[0mlast_unique\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1083\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1084\u001B[0m         \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\amato\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[1;34m(etype, value, records)\u001B[0m\n\u001B[0;32m    380\u001B[0m     \u001B[1;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    381\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 382\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    383\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    384\u001B[0m     \u001B[1;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "already_connected_mariadb = False\n",
    "# Connect to MariaDB Platform\n",
    "if not already_connected_mariadb:\n",
    "    try:\n",
    "        conn = mariadb.connect(\n",
    "            user=\"root\",\n",
    "            password=\"armando12\",\n",
    "            host=\"localhost\",\n",
    "            port=3306,\n",
    "            database=\"maadb_tweets\"\n",
    "        )\n",
    "    except mariadb.Error as e:\n",
    "        print(f\"Error connecting to MariaDB Platform: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Get Cursor\n",
    "    cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SHOW TABLES\")\n",
    "\n",
    "for (table_name,) in cur:\n",
    "    print(table_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MariaDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connection to MariaDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}